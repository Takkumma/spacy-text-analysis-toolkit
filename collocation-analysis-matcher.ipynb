{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 共起語分析スクリプト(Matcher)\n",
    "\n",
    "このスクリプトはspaCyライブラリを使用して事前に解析されたテキストデータに対して共起語分析を行う。文単位での共起語分析、共起範囲指定による分析、例文抽出機能などが含まれる。\n",
    "\n",
    "## 使用方法\n",
    "\n",
    "### 解析データの読み込み\n",
    "\n",
    "1. 解析済みのテキストデータ（.spacyファイル）とそれに対応するテキストファイル（.txt）を用意する。\n",
    "2. スクリプト内の「ユーザーが設定するパラメーター」セクションで以下の変数を設定する。\n",
    " - `main_directory`: 親ディレクトリへの相対パス  \n",
    " - `subdirectories`: 読み込む子ディレクトリ名のリスト \n",
    " - `nlp_model`: 解析に利用したモデル名\n",
    " - `selected_file_names`: 特定のファイルのみを読み込む場合はここにtxtファイル名を追加（空リストの場合は全ファイルを読み込む）\n",
    "\n",
    "### 共起語分析（文単位）の実行\n",
    "\n",
    "1. スクリプト内の「ユーザーが設定するパラメーター」セクションで以下の変数を設定する。\n",
    " - `pattern`: キーワードの検索パターンを指定\n",
    " - `output_pos`: フィルターする品詞のリスト\n",
    " - `max_output`: 出力する結果の最大数\n",
    " - `output_file`: 共起語分析の結果を保存するファイル名\n",
    "2. スクリプトを実行すると、指定したファイル名でCSVファイルが出力される。\n",
    "\n",
    "### 共起語分析（共起範囲指定）の実行\n",
    "\n",
    "1. スクリプト内の「ユーザーが設定するパラメーター」セクションで以下の変数を設定する。\n",
    " - `pattern`: キーワードの検索パターンを指定\n",
    " - `range_left`, `range_right`: 共起語の左右の範囲を指定\n",
    " - `output_pos`: フィルターする品詞のリスト\n",
    " - `max_output`: 出力する結果の最大数\n",
    " - `lowercase`: キーワードの集計を小文字で行うかどうか\n",
    " - `output_file`: 共起語分析の結果を保存するファイル名\n",
    "2. スクリプトを実行すると、指定したファイル名でCSVファイルが出力される。\n",
    "\n",
    "### キーワードや周辺トークンの集計\n",
    "\n",
    "1. 共起語分析（共起範囲指定）の実行後、キーワードとその周辺のトークンの相対インデックスごとの出現回数が集計される。\n",
    "2. 集計結果は`output_filename`で指定したCSVファイルに出力される。\n",
    "\n",
    "### 例文抽出機能の実行\n",
    "\n",
    "1. スクリプト内の「ユーザーが設定するパラメーター」セクションで以下の変数を設定する。\n",
    " - `lemma_to_extract`: 抽出する単語を指定\n",
    " - `pos_to_extract`: 抽出する単語の品詞を指定\n",
    " - `output_text_file`: 抽出した文を保存するファイル名 \n",
    "2. スクリプトを実行すると、指定した単語と品詞に対応する例文がテキストファイルに保存される。\n",
    "\n",
    "## 分析内容\n",
    "\n",
    "### 1. 共起語分析（文単位） \n",
    "\n",
    "指定したキーワードパターンに基づいて、文単位での共起語を分析する。結果はCSVファイルとして出力され、全体の共起語と指定した品詞ごとの共起語が含まれる。\n",
    "\n",
    "### 2. 共起語分析（共起範囲指定）\n",
    "\n",
    "キーワードを中心に指定した範囲内の共起語を分析する。結果はCSVファイルとして出力され、全体の共起語と指定した品詞ごとの共起語が含まれる。また、キーワードとその周辺のトークンの相対インデックスごとの出現回数が集計され、CSVファイルに出力される。\n",
    "\n",
    "### 3. キーワードの周辺トークンの集計\n",
    "共起語分析（共起範囲指定）の結果から、キーワードとその周辺のトークンの相対インデックスごとの出現回数を集計する。集計結果はCSVファイルとして出力され、各相対インデックスにおけるトークンとその出現頻度が含まれる。\n",
    "\n",
    "### 4. 例文抽出機能\n",
    "\n",
    "共起語分析の結果から、指定した単語と品詞に対応する例文を抽出し、テキストファイルとして保存する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ライブラリのインポート(最初に実行)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.matcher import Matcher\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **解析データの読み込み**\n",
    "## ユーザーが設定するパラメーター"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本設定\n",
    "main_directory = \"processed_data\"  # 親ディレクトリへの相対パス\n",
    "subdirectories = [\"wiki_english\"]  # 読み込む子ディレクトリ名のリスト\n",
    "nlp_model = \"en_core_web_sm\"  # 解析に利用したモデル名\n",
    "\n",
    "# 特定のファイルのみを読み込む場合は以下のリストにtxtファイル名を追加(空リストの場合は全ファイルを読み込む)\n",
    "selected_file_names = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解析データ読み込みの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCyの言語モデルをロード\n",
    "nlp = spacy.load(nlp_model)\n",
    "\n",
    "# 解析結果を格納する辞書の初期化\n",
    "docs_dict = {}\n",
    "# フィルタリングされたDocオブジェクトを格納する辞書の初期化\n",
    "filtered_docs = {}\n",
    "\n",
    "# 指定されたサブディレクトリ内のファイルを読み込む\n",
    "for subdir in subdirectories:\n",
    "    directory = os.path.join(main_directory, subdir)\n",
    "    \n",
    "    # サブディレクトリ内のファイルを読み込む\n",
    "    for filename in os.listdir(directory): \n",
    "        if filename.endswith(\".spacy\"):  # .spacyファイルならば\n",
    "            spacy_path = os.path.join(directory, filename)\n",
    "            txt_filename = filename.replace(\".spacy\", \".txt\")  # .spaCyファイルに対応するテキストファイル名を作成\n",
    "            txt_path = os.path.join(directory, txt_filename)  # .spacyファイルに対応するテキストファイルのパス\n",
    "            \n",
    "            # .spacyファイルからDocオブジェクトを読み込む\n",
    "            doc_bin = DocBin().from_disk(spacy_path)  # DocBinオブジェクトを読み込む\n",
    "            docs = list(doc_bin.get_docs(nlp.vocab))  # Docオブジェクトをリストに格納\n",
    "            \n",
    "            # 対応する.txtファイルからファイル名を読み込む\n",
    "            if os.path.exists(txt_path):\n",
    "                with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "                    file_names = [line.strip() for line in f.readlines()]  # ファイル名をリストに格納\n",
    "                \n",
    "                # Docオブジェクトとファイル名の対応を辞書に格納\n",
    "                for doc, fname in zip(docs, file_names):\n",
    "                    docs_dict[fname] = doc\n",
    "            else:\n",
    "                print(f\"警告: {txt_filename} に対応するテキストファイルが存在しません。\")\n",
    "\n",
    "# ユーザーがファイル名を指定した場合、それに対応するDocオブジェクトをフィルタリング\n",
    "if selected_file_names:\n",
    "    missing_files = []  # 存在しないファイル名を格納するリスト\n",
    "    for fname in selected_file_names:\n",
    "        if fname in docs_dict:  # 指定されたファイル名が読み込んだデータに存在する場合\n",
    "            filtered_docs[fname] = docs_dict[fname]\n",
    "        else:\n",
    "            missing_files.append(fname)\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"警告: 次の指定されたファイルは存在しません: {', '.join(missing_files)}\")\n",
    "    if filtered_docs:\n",
    "        print(\"指定された一部のデータが読み込まれました。\")\n",
    "    else:\n",
    "        print(\"指定されたファイルに対応するデータが見つかりませんでした。\")\n",
    "else:\n",
    "    filtered_docs = docs_dict\n",
    "    print(\"全てのデータが読み込まれました。\")\n",
    "print(f\"読み込まれたデータ数: {len(filtered_docs)}\")\n",
    "print(f\"読み込まれたファイル名: {list(filtered_docs.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **共起語分析（文単位）**\n",
    "## ユーザーが設定するパラメーター"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# キーワードの検索パターンを指定\n",
    "pattern = [{\"LOWER\": \"hello\"}, {\"LOWER\": \"world\"}]\n",
    "\n",
    "# 出力設定\n",
    "output_pos = [\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\", \"ADV\"]   # フィルターする品詞をリストで設定\n",
    "max_output = 150  # 出力する結果の最大数を設定\n",
    "output_file = \"sentence_level_collocations.csv\"  # 出力ファイル名"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 共起語分析の実行（文単位）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_collocation_search(doc, pattern, collocation_dict, file_name):\n",
    "    \"\"\"\n",
    "    指定されたパターンに基づいて、文単位での共起語を検索し、共起語の情報を辞書に記録する。\n",
    "\n",
    "    Args:\n",
    "        doc (Doc): 解析対象のDocオブジェクト\n",
    "        pattern (list): 検索パターンのリスト\n",
    "        collocation_dict (defaultdict): 共起語情報を格納する辞書\n",
    "        file_name (str): 解析対象のファイル名\n",
    "    \"\"\"\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"PATTERN\", [pattern])\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    for sent in doc.sents:  # 文ごとに処理\n",
    "        spans = [(start, end) for match_id, start, end in matches if sent.start <= start < sent.end]\n",
    "\n",
    "        if spans:  # キーワードが含まれる文のみ処理\n",
    "            # キーワードに一致するスパンを除外してトークンをフィルタリング\n",
    "            filtered_tokens = [token for token in sent if not any(span[0] <= token.i < span[1] for span in spans)]\n",
    "            record_collocations(filtered_tokens, collocation_dict, file_name)\n",
    "\n",
    "def record_collocations(tokens, collocation_dict, file_name):\n",
    "    \"\"\"\n",
    "    共起語の情報を辞書に記録する。\n",
    "\n",
    "    Args:\n",
    "        tokens (list): 共起語のトークンのリスト\n",
    "        collocation_dict (defaultdict): 共起語情報を格納する辞書\n",
    "        file_name (str): 解析対象のファイル名\n",
    "    \"\"\"\n",
    "    for token in tokens:\n",
    "        key = (token.lemma_, token.pos_)  # 共起語のキーとして、基本形と品詞を組み合わせる\n",
    "        collocation_dict[key]['lemma'] = token.lemma_  \n",
    "        collocation_dict[key]['pos'] = token.pos_  \n",
    "        collocation_dict[key]['freq'] += 1  \n",
    "        collocation_dict[key]['indexes'].append((file_name, token.i)) \n",
    "\n",
    "\n",
    "def save_collocations_to_csv(collocation_dict, output_file, output_pos=None, max_output=None):\n",
    "    \"\"\"\n",
    "    共起語の情報をCSVファイルに保存する。\n",
    "\n",
    "    Args:\n",
    "        collocation_dict (defaultdict): 共起語情報を格納する辞書\n",
    "        output_file (str): 出力ファイル名\n",
    "        output_pos (list, optional): 出力する品詞のリスト。デフォルトはNone。\n",
    "        max_output (int, optional): 出力する結果の最大数。デフォルトはNone。\n",
    "    \"\"\"\n",
    "    # 辞書からデータフレームを作成\n",
    "    df = pd.DataFrame.from_dict(collocation_dict, orient='index')\n",
    "    \n",
    "    # 全品詞に対するソート\n",
    "    df_sorted_all = df.sort_values(by='freq', ascending=False)\n",
    "    if max_output is not None:\n",
    "        df_sorted_all = df_sorted_all.head(max_output)\n",
    "    \n",
    "    # 指定された品詞でフィルタリング\n",
    "    dfs_filtered = []\n",
    "    if output_pos:\n",
    "        for pos in output_pos:\n",
    "            df_filtered = df[df['pos'] == pos].sort_values(by='freq', ascending=False)\n",
    "            if max_output is not None:\n",
    "                df_filtered = df_filtered.head(max_output)\n",
    "            df_filtered = df_filtered[['lemma', 'freq']].reset_index(drop=True)\n",
    "            df_filtered.columns = [f'lemma_{pos}', f'freq_{pos}']\n",
    "            dfs_filtered.append(df_filtered)\n",
    "    \n",
    "    # 指定された品詞に基づくデータフレームを結合\n",
    "    df_filtered_combined = pd.concat(dfs_filtered, axis=1)\n",
    "    # 全品詞の共起語データを追加\n",
    "    df_final = pd.concat([df_filtered_combined, df_sorted_all[['lemma', 'pos', 'freq']].reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    # CSVとして保存\n",
    "    df_final.to_csv(output_file, index=False)\n",
    "\n",
    "# 共起語を格納するための辞書の初期化\n",
    "collocation_dict = defaultdict(lambda: {'lemma': '', 'pos': '', 'freq': 0, 'indexes': []})\n",
    "\n",
    "# 全てのDocオブジェクトに対して共起語分析を実行\n",
    "for file_name, doc in filtered_docs.items():\n",
    "    perform_collocation_search(doc, pattern, collocation_dict, file_name) # 共起語分析を実行\n",
    "\n",
    "# 共起語の総数を計算し、結果を表示\n",
    "total_matches = len(collocation_dict) # 共起語の総数を計算\n",
    "if total_matches > 0:\n",
    "    print(f\"分析結果: {total_matches} 件の共起語が見つかりました。\")\n",
    "    save_collocations_to_csv(collocation_dict, output_file, output_pos, max_output)  # 結果をCSVファイルに保存\n",
    "    print(f\"結果が {output_file} に保存されました。\")\n",
    "else:\n",
    "    print(\"結果が見つかりませんでした。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 例文抽出機能\n",
    "## ユーザーが設定するパラメーター"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_to_extract = 'earth'  # 抽出する単語を指定\n",
    "pos_to_extract = 'NOUN'  # 抽出する単語の品詞を指定\n",
    "output_text_file = 'extract_file.txt'  # 抽出した文を保存するファイル名"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例文抽出の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_by_collocation(filtered_docs, collocation_dict, lemma, pos, output_file):\n",
    "    \"\"\"\n",
    "    指定された共起語と品詞に対応する文を抽出し、テキストファイルに保存する。\n",
    "\n",
    "    Args:\n",
    "        filtered_docs (dict): フィルタリングされたDocオブジェクトの辞書\n",
    "        collocation_dict (defaultdict): 共起語情報を格納する辞書(例 : {(lemma, pos): {'lemma': '', 'pos': '', 'freq': 0, 'indexes': []}, ...}\n",
    "        lemma (str): 抽出する単語のレンマ\n",
    "        pos (str): 抽出する単語の品詞\n",
    "        output_file (str): 抽出した文を保存するファイル名\n",
    "    \"\"\"\n",
    "    # 共起語と品詞に対応するインデックスを取得(例 : indexes = [(file_name, token_index), ...]\n",
    "    indexes = collocation_dict.get((lemma, pos), {}).get('indexes', [])\n",
    "    if not indexes:\n",
    "        print(f\"警告: 指定された共起語 ({lemma}, {pos}) に対応する例文が見つかりませんでした。\")\n",
    "        return\n",
    "    \n",
    "    # 処理された文を追跡するためのセット\n",
    "    processed_sentences = set()\n",
    "\n",
    "    # ドキュメントとテキスト情報のペアを用いて実際の文を抽出\n",
    "    sentences = []\n",
    "    for file_name, doc in filtered_docs.items():\n",
    "        for token in doc:\n",
    "            # トークンのインデックスと文書のテキスト情報を組み合わせたキーを作成\n",
    "            token_key = (file_name, token.i)\n",
    "            if token_key in indexes:\n",
    "                sentence_text = token.sent.text\n",
    "                # 重複を避けるために、すでに処理された文をチェック\n",
    "                if sentence_text not in processed_sentences:\n",
    "                    sentences.append(sentence_text)\n",
    "                    processed_sentences.add(sentence_text)\n",
    "    \n",
    "    # テキストファイルに1行1文の形で出力\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence + '\\n')\n",
    "    print(f\"抽出された文({len(sentences)}件)が {output_file} に保存されました。\")\n",
    "\n",
    "# 文を抽出してテキストファイルに保存\n",
    "extract_sentences_by_collocation(filtered_docs, collocation_dict, lemma_to_extract, pos_to_extract, output_text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **共起語分析（共起範囲指定）**\n",
    "## ユーザーが設定するパラメーター"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# キーワードの検索パターンを指定\n",
    "pattern = [{\"LOWER\": \"hello\"}, {\"LOWER\": \"world\"}]\n",
    "\n",
    "range_left = 15    # 共起語の左側の範囲を指定\n",
    "range_right = 15   # 共起語の右側の範囲を指定\n",
    "\n",
    "# 出力設定\n",
    "output_pos = [\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\", \"ADV\"]   # フィルターする品詞をリストで設定\n",
    "max_output = 150  # 出力する結果の最大数を設定\n",
    "lowercase = True  # 「キーワードや周辺トークンの集計」を小文字で統一するか(True/False)\n",
    "output_file = \"collocations_within_range.csv\"  # 出力ファイル名"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 共起語分析の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_with_window(doc, pattern, collocation_dict, file_name, range_left, range_right, lowercase=True):\n",
    "    \"\"\"\n",
    "    指定された範囲内で共起語を検索し、共起語の情報を辞書に記録する。\n",
    "\n",
    "    Args:\n",
    "        doc (Doc): 解析対象のDocオブジェクト\n",
    "        pattern (list): 検索パターンのリスト\n",
    "        collocation_dict (defaultdict): 共起語情報を格納する辞書\n",
    "        file_name (str): 解析対象のファイル名\n",
    "        range_left (int): 共起語の左側の範囲\n",
    "        range_right (int): 共起語の右側の範囲\n",
    "        lowercase (bool): キーワードの集計を小文字で行うかどうか。デフォルトはTrue。\n",
    "\n",
    "    Returns:\n",
    "        defaultdict: トークンの相対インデックスごとの出現回数をカウントする辞書\n",
    "    \"\"\"\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"PATTERN\", [pattern])\n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    # token_index_countsを初期化\n",
    "    token_index_counts = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    # マッチした各部分に対する処理\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]  # マッチした部分のスパン\n",
    "        sentence = span.sent  # スパンが含まれる文\n",
    "        \n",
    "        # マッチしたフレーズの開始トークンと終了トークンを基準にウィンドウサイズを適用\n",
    "        window_start = max(span.start - range_left, sentence.start) # 文の先頭を超えないようにする\n",
    "        window_end = min(span.end + range_right, sentence.end) # 文の末尾を超えないようにする\n",
    "        \n",
    "        # ウィンドウ内のトークンを抽出（キーワードを除く）\n",
    "        window_tokens = [sentence[i - sentence.start] for i in range(window_start, window_end) if i < span.start or i >= span.end]\n",
    "        \n",
    "        # フィルタリングされたトークンのリストを使用して共起語を記録\n",
    "        record_collocations(window_tokens, collocation_dict, file_name)\n",
    "        \n",
    "        # 左側の範囲内のトークンをカウント\n",
    "        for i, token in enumerate(doc[max(start - range_left, 0):start], start=1):\n",
    "            token_text = token.text.lower() if lowercase else token.text\n",
    "            token_index_counts[f\"L{range_left - i + 1}\"][token_text] += 1\n",
    "\n",
    "        # キーワードをカウント\n",
    "        for i, token in enumerate(doc[start:end], start=1):\n",
    "            token_text = token.text.lower() if lowercase else token.text\n",
    "            token_index_counts[f\"K{i}\"][token_text] += 1\n",
    "        \n",
    "        # 右側の範囲内のトークンをカウント\n",
    "        for i, token in enumerate(doc[end:min(end + range_right, len(doc))], start=1):\n",
    "            token_text = token.text.lower() if lowercase else token.text\n",
    "            token_index_counts[f\"R{i}\"][token_text] += 1\n",
    "    \n",
    "    return token_index_counts\n",
    "\n",
    "def record_collocations(tokens, collocation_dict, file_name):\n",
    "    \"\"\"\n",
    "    共起語の情報を辞書に記録する。\n",
    "\n",
    "    Args:\n",
    "        tokens (list): 共起語のトークンのリスト\n",
    "        collocation_dict (defaultdict): 共起語情報を格納する辞書\n",
    "        file_name (str): 解析対象のファイル名\n",
    "    \"\"\"\n",
    "    for token in tokens:\n",
    "        key = (token.lemma_, token.pos_)  # トークンのレンマと品詞をキーとして使用\n",
    "        collocation_dict[key]['lemma'] = token.lemma_  # トークンのレンマを記録\n",
    "        collocation_dict[key]['pos'] = token.pos_  # トークンの品詞を記録\n",
    "        collocation_dict[key]['freq'] += 1  # 出現頻度を更新\n",
    "        collocation_dict[key]['indexes'].append((file_name, token.i))  # トークンの出現位置を記録\n",
    "\n",
    "def save_collocations_to_csv(collocation_dict, output_file, output_pos=None, max_output=None):\n",
    "    \"\"\"\n",
    "    共起語の情報をCSVファイルに保存する。\n",
    "\n",
    "    Args:\n",
    "        collocation_dict (defaultdict): 共起語情報を格納する辞書\n",
    "        output_file (str): 出力ファイル名\n",
    "        output_pos (list, optional): 出力する品詞のリスト。デフォルトはNone。\n",
    "        max_output (int, optional): 出力する結果の最大数。デフォルトはNone。\n",
    "    \"\"\"\n",
    "    # 辞書からデータフレームを作成\n",
    "    df = pd.DataFrame.from_dict(collocation_dict, orient='index')\n",
    "    \n",
    "    # 全品詞に対するソート\n",
    "    df_sorted_all = df.sort_values(by='freq', ascending=False)\n",
    "    if max_output is not None:\n",
    "        df_sorted_all = df_sorted_all.head(max_output)\n",
    "    \n",
    "    # 指定された品詞でフィルタリング\n",
    "    dfs_filtered = []\n",
    "    if output_pos:\n",
    "        for pos in output_pos:\n",
    "            df_filtered = df[df['pos'] == pos].sort_values(by='freq', ascending=False)\n",
    "            if max_output is not None:\n",
    "                df_filtered = df_filtered.head(max_output)\n",
    "            df_filtered = df_filtered[['lemma', 'freq']].reset_index(drop=True)\n",
    "            df_filtered.columns = [f'lemma_{pos}', f'freq_{pos}']\n",
    "            dfs_filtered.append(df_filtered)\n",
    "    \n",
    "    # 指定された品詞に基づくデータフレームを結合\n",
    "    df_filtered_combined = pd.concat(dfs_filtered, axis=1)\n",
    "    # 全品詞の共起語データを追加\n",
    "    df_final = pd.concat([df_filtered_combined, df_sorted_all[['lemma', 'pos', 'freq']].reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    # CSVとして保存\n",
    "    df_final.to_csv(output_file, index=False)\n",
    "\n",
    "# 共起語を格納するための辞書の初期化\n",
    "collocation_dict = defaultdict(lambda: {'lemma': '', 'pos': '', 'freq': 0, 'indexes': []})\n",
    "all_token_index_counts = defaultdict(lambda: defaultdict(int))  # すべてのドキュメントのトークンのインデックスごとのカウント\n",
    "\n",
    "# 各文書に対する共起語の検索と記録\n",
    "for file_name, doc in filtered_docs.items():\n",
    "    token_index_counts = search_with_window(doc, pattern, collocation_dict, file_name, range_left, range_right)\n",
    "    # トークンの相対インデックスごとのカウントをマージ\n",
    "    for index, counts in token_index_counts.items():\n",
    "        for token, count in counts.items():\n",
    "            all_token_index_counts[index][token] += count\n",
    "\n",
    "# 共起語の総数を計算し、結果を表示\n",
    "total_matches = len(collocation_dict)  # 共起語の総数を計算\n",
    "if total_matches > 0:\n",
    "    print(f\"分析結果: {total_matches} 件の共起語が見つかりました。\")\n",
    "    save_collocations_to_csv(collocation_dict, output_file, output_pos, max_output)  # 結果をCSVファイルに保存\n",
    "    print(f\"結果が {output_file} に保存されました。\")\n",
    "else:\n",
    "    print(\"結果が見つかりませんでした。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## キーワードや周辺トークンの集計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出力するCSVファイルの名前を定義\n",
    "output_filename = \"indexed_token_counts.csv\"\n",
    "\n",
    "def output_indexed_token_counts(token_index_counts, output_filename):\n",
    "    \"\"\"\n",
    "    トークンの相対インデックスごとの集計結果をデータフレームに変換し、CSVファイルに出力する。\n",
    "\n",
    "    Args:\n",
    "        token_index_counts (defaultdict): トークンの相対インデックスごとの出現回数をカウントする辞書。\n",
    "        output_filename (str): 出力するCSVファイルの名前。\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: トークンの相対インデックスごとの集計結果を含むデータフレーム。\n",
    "    \"\"\"\n",
    "\n",
    "    # 集計データをリストに変換する\n",
    "    data = []\n",
    "    for index, tokens in token_index_counts.items():\n",
    "        for token, count in tokens.items():\n",
    "            data.append({\"Index\": index, \"Token\": token, \"Count\": count})\n",
    "\n",
    "    # リストからデータフレームを作成し、インデックスとカウントでソート\n",
    "    df = pd.DataFrame(data)\n",
    "    df_sorted = df.sort_values(by=[\"Index\", \"Count\"], ascending=[True, False])\n",
    "\n",
    "    # 最終的な結果を格納するための空のデータフレームを作成\n",
    "    result_df = pd.DataFrame()\n",
    "\n",
    "    # 相対インデックスの順序を指定\n",
    "    index_order = [f\"L{i}\" for i in range(range_left, 0, -1)] + [f\"K{i}\" for i in range(1, 6)] + [f\"R{i}\" for i in range(1, range_right + 1)]\n",
    "\n",
    "    # 指定された順序で相対インデックスごとにデータフレームを作成し、最終結果に追加\n",
    "    for index in index_order:\n",
    "        if index in set(df_sorted[\"Index\"]):\n",
    "            temp_df = df_sorted[df_sorted[\"Index\"] == index].copy()\n",
    "            temp_df.drop(\"Index\", axis=1, inplace=True)  # インデックス列は不要なので削除\n",
    "            temp_df.columns = [f\"Token_{index}\", f\"Freq_{index}\"]  # 列名を設定\n",
    "            result_df = pd.concat([result_df, temp_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # 最終的な結果をCSVファイルに出力\n",
    "    result_df.to_csv(output_filename, index=False)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# 関数の実行とCSVファイルへの出力\n",
    "result_df = output_indexed_token_counts(all_token_index_counts, output_filename)\n",
    "\n",
    "print(f\"トークンのインデックス別集計結果を {output_filename} に出力しました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 例文抽出機能\n",
    "## ユーザーが設定するパラメータ―"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_to_extract = 'earth' # 抽出する単語を指定 \n",
    "pos_to_extract = 'NOUN'  # 抽出する単語の品詞を指定\n",
    "output_text_file = 'extract_file.txt'  # 抽出した文を保存するファイル名"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例文抽出の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_by_collocation(filtered_docs, collocation_dict, lemma, pos, output_file):\n",
    "    \"\"\"\n",
    "    指定された共起語と品詞に対応する文を抽出し、テキストファイルに保存する。\n",
    "\n",
    "    Args:\n",
    "        filtered_docs (dict): フィルタリングされたDocオブジェクトの辞書\n",
    "        collocation_dict (defaultdict): 共起語情報を格納する辞書(例 : {(lemma, pos): {'lemma': '', 'pos': '', 'freq': 0, 'indexes': []}, ...}\n",
    "        lemma (str): 抽出する単語のレンマ\n",
    "        pos (str): 抽出する単語の品詞\n",
    "        output_file (str): 抽出した文を保存するファイル名\n",
    "    \"\"\"\n",
    "    # 共起語と品詞に対応するインデックスを取得(例 : indexes = [(file_name, token_index), ...]\n",
    "    indexes = collocation_dict.get((lemma, pos), {}).get('indexes', [])\n",
    "    if not indexes:\n",
    "        print(f\"警告: 指定された共起語 ({lemma}, {pos}) に対応する例文が見つかりませんでした。\")\n",
    "        return\n",
    "    \n",
    "    # 処理された文を追跡するためのセット\n",
    "    processed_sentences = set()\n",
    "\n",
    "    # ドキュメントとテキスト情報のペアを用いて実際の文を抽出\n",
    "    sentences = []\n",
    "    for file_name, doc in filtered_docs.items():\n",
    "        for token in doc:\n",
    "            # トークンのインデックスと文書のテキスト情報を組み合わせたキーを作成\n",
    "            token_key = (file_name, token.i)\n",
    "            if token_key in indexes:\n",
    "                sentence_text = token.sent.text\n",
    "                # 重複を避けるために、すでに処理された文をチェック\n",
    "                if sentence_text not in processed_sentences:\n",
    "                    sentences.append(sentence_text)\n",
    "                    processed_sentences.add(sentence_text)\n",
    "    \n",
    "    # テキストファイルに1行1文の形で出力\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence + '\\n')\n",
    "    print(f\"抽出された文({len(sentences)}件)が {output_file} に保存されました。\")\n",
    "\n",
    "# 文を抽出してテキストファイルに保存\n",
    "extract_sentences_by_collocation(filtered_docs, collocation_dict, lemma_to_extract, pos_to_extract, output_text_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kasou",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
