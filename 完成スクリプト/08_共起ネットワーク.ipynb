{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 共起ネットワーク分析スクリプト\n",
    "\n",
    "このスクリプトは、事前に解析されたテキストデータ（.spacyファイル）を読み込み、共起行列を作成して共起ネットワークを構築する。得られたネットワークは、Pyvisライブラリを用いてインタラクティブに可視化される。また、ネットワークのノードとエッジの情報がCSVファイルとして出力される。\n",
    "\n",
    "## 主な機能\n",
    "\n",
    "1. 解析済みのテキストデータ（.spacyファイル）とそれに対応するテキストファイル（.txt）の読み込み\n",
    "2. 共起行列の作成\n",
    "  - ユーザー指定の出現回数の範囲内でのフィルタリング\n",
    "  - トークンの最小長の指定\n",
    "3. エッジの重み計算\n",
    "  - 共起頻度、Jaccard係数、Dice係数、Simpson係数から選択\n",
    "4. 共起ネットワークの構築\n",
    "  - エッジの重みの範囲指定\n",
    "  - 出力するエッジ数の指定\n",
    "  - 独立したエッジの除去\n",
    "  - コミュニティ検出\n",
    "  - ノードの重み計算（中心性または出現頻度）\n",
    "5. ネットワークの可視化（Pyvisライブラリ）\n",
    "  - ノードとエッジのサイズ調整\n",
    "  - ノードの属性情報の表示\n",
    "  - インタラクティブな操作\n",
    "6. ネットワークのノードとエッジ情報のCSV出力\n",
    "\n",
    "## 使用方法\n",
    "\n",
    "1. 必要なライブラリをインストールする（`spacy`, `networkx`, `pyvis`など）\n",
    "2. 解析済みのテキストデータ（.spacyファイル）とそれに対応するテキストファイル（.txt）を用意する\n",
    "3. スクリプト内の「ユーザーが設定するパラメーター」セクションで以下の変数を設定する\n",
    "  - `main_directory`: 親ディレクトリへの相対パス\n",
    "  - `subdirectories`: 読み込む子ディレクトリ名のリスト\n",
    "  - `nlp_model`: 解析に利用したモデル名\n",
    "  - `selected_file_names`: 特定のファイルのみを読み込む場合はここにtxtファイル名を追加（空リストの場合は全ファイルを読み込む）\n",
    "4. 共起行列の作成に関するパラメーターを設定する\n",
    "  - `min_freq`, `max_freq`: 出現回数の下限と上限\n",
    "  - `token_length`: トークンの最小長\n",
    "5. エッジの重み計算方法を選択する\n",
    "  - `weight_mode`: 'cooccurrence', 'jaccard', 'dice', 'simpson'から選択\n",
    "6. 共起ネットワークの構築に関するパラメーターを設定する\n",
    "  - `weight_lower_cutoff`, `weight_upper_cutoff`: エッジの重みの範囲\n",
    "  - `num_edges_to_output`: 出力するエッジの数（Noneの場合は全て出力）\n",
    "  - `max_degree`: カットオフする独立したエッジの数（Noneの場合は全て出力）\n",
    "  - `node_multiplier`, `edge_multiplier`: ノードとエッジのサイズ調整係数\n",
    "   - `node_weight_mode`: ノードの重みを計算する方法（`'centrality'` または `'frequency'`）\n",
    "     - `'centrality'`: ネットワーク内でのノードの重要度（中心性）に基づく\n",
    "     - `'frequency'`: テキスト内での単語の出現頻度に基づく\n",
    "7. スクリプトを実行する\n",
    "8. 出力されたHTMLファイル（`network.html`）をウェブブラウザで開き、インタラクティブなネットワーク可視化を explore する\n",
    "9. 出力されたCSVファイル（`network_edges.csv`, `network_nodes.csv`）を必要に応じて他のツールで分析する\n",
    "\n",
    "## 注意事項\n",
    "\n",
    "- 大規模なデータセットを扱う場合、処理に時間がかかる可能性がある\n",
    "- パラメーターの設定によっては、ネットワークが複雑になりすぎて解釈が困難になる場合がある\n",
    "- このスクリプトは、事前に解析されたテキストデータ（.spacyファイル）を必要とする。生のテキストデータを扱う場合は、別途前処理が必要である"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ライブラリのインポート(最初に実行)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **解析データの読み込み**\n",
    "## ユーザーが設定するパラメーター"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本設定\n",
    "main_directory = \"processed_data\"  # 親ディレクトリへの相対パス\n",
    "subdirectories = [\"sub1\"]  # 読み込む子ディレクトリ名のリスト\n",
    "nlp_model = \"en_core_web_sm\"  # 解析に利用したモデル名\n",
    "\n",
    "# 特定のファイルのみを読み込む場合は以下のリストにtxtファイル名を追加(空リストの場合は全ファイルを読み込む)\n",
    "selected_file_names = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解析データ読み込みの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCyの言語モデルをロード\n",
    "nlp = spacy.load(nlp_model)\n",
    "\n",
    "# 解析結果を格納する辞書の初期化\n",
    "docs_dict = {}\n",
    "# フィルタリングされたDocオブジェクトを格納する辞書の初期化\n",
    "filtered_docs = {}\n",
    "\n",
    "# 指定されたサブディレクトリ内のファイルを読み込む\n",
    "for subdir in subdirectories:\n",
    "    directory = os.path.join(main_directory, subdir)\n",
    "    \n",
    "    # サブディレクトリ内のファイルを読み込む\n",
    "    for filename in os.listdir(directory): \n",
    "        if filename.endswith(\".spacy\"):  # .spacyファイルならば\n",
    "            spacy_path = os.path.join(directory, filename)\n",
    "            txt_filename = filename.replace(\".spacy\", \".txt\")  # .spaCyファイルに対応するテキストファイル名を作成\n",
    "            txt_path = os.path.join(directory, txt_filename)  # .spacyファイルに対応するテキストファイルのパス\n",
    "            \n",
    "            # .spacyファイルからDocオブジェクトを読み込む\n",
    "            doc_bin = DocBin().from_disk(spacy_path)  # DocBinオブジェクトを読み込む\n",
    "            docs = list(doc_bin.get_docs(nlp.vocab))  # Docオブジェクトをリストに格納\n",
    "            \n",
    "            # 対応する.txtファイルからファイル名を読み込む\n",
    "            if os.path.exists(txt_path):\n",
    "                with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "                    file_names = [line.strip() for line in f.readlines()]  # ファイル名をリストに格納\n",
    "                \n",
    "                # Docオブジェクトとファイル名の対応を辞書に格納\n",
    "                for doc, fname in zip(docs, file_names):\n",
    "                    docs_dict[fname] = doc\n",
    "            else:\n",
    "                print(f\"警告: {txt_filename} に対応するテキストファイルが存在しません。\")\n",
    "\n",
    "# ユーザーがファイル名を指定した場合、それに対応するDocオブジェクトをフィルタリング\n",
    "if selected_file_names:\n",
    "    missing_files = []  # 存在しないファイル名を格納するリスト\n",
    "    for fname in selected_file_names:\n",
    "        if fname in docs_dict:  # 指定されたファイル名が読み込んだデータに存在する場合\n",
    "            filtered_docs[fname] = docs_dict[fname]\n",
    "        else:\n",
    "            missing_files.append(fname)\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"警告: 次の指定されたファイルは存在しません: {', '.join(missing_files)}\")\n",
    "    if filtered_docs:\n",
    "        print(\"指定された一部のデータが読み込まれました。\")\n",
    "    else:\n",
    "        print(\"指定されたファイルに対応するデータが見つかりませんでした。\")\n",
    "else:\n",
    "    filtered_docs = docs_dict\n",
    "    print(\"全てのデータが読み込まれました。\")\n",
    "print(f\"読み込まれたデータ数: {len(filtered_docs)}\")\n",
    "print(f\"読み込まれたファイル名: {list(filtered_docs.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **テキストデータの前処理（品詞指定, ストップワード除去）**\n",
    "## ユーザーが設定するパラメーター"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽出する品詞リストを設定\n",
    "include_pos = [\"NOUN\", \"PROPN\", \"ADJ\", \"ADV\"]\n",
    "\n",
    "# ストップワードの設定\n",
    "custom_stopwords_path = 'stopwords.txt'  # ユーザー定義のストップワードファイルのパス\n",
    "use_spacy_stopwords = True  # spacyのデフォルトストップワードを使用するかどうか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(file_path):\n",
    "    \"\"\"\n",
    "    ストップワードをファイルから読み込む関数\n",
    "\n",
    "    Args:\n",
    "        file_path (str): ストップワードが記載されたファイルのパス\n",
    "\n",
    "    Returns:\n",
    "        set: ストップワードの集合\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            stopwords = set(file.read().splitlines())\n",
    "    else:\n",
    "        stopwords = set()\n",
    "        print(\"警告: ストップワードファイルが見つかりません。ストップワードは使用されません。\")\n",
    "    return stopwords\n",
    "\n",
    "def extract_words(sent, pos_tags, stopwords):\n",
    "    \"\"\"\n",
    "    文から指定した品詞のトークンを抽出し、ストップワードを除去する関数\n",
    "\n",
    "    Args:\n",
    "        sent (spacy.tokens.span.Span): spaCyの文オブジェクト\n",
    "        pos_tags (list): 抽出対象の品詞リスト\n",
    "        stopwords (set): ストップワードの集合\n",
    "\n",
    "    Returns:\n",
    "        list: 抽出されたトークンのリスト\n",
    "    \"\"\"\n",
    "    words = [token.lemma_ for token in sent if token.pos_ in pos_tags and token.lemma_ not in stopwords]\n",
    "    return words\n",
    "\n",
    "all_sents = []  # 前処理後の文を格納するリスト\n",
    "spacy_stopwords = nlp.Defaults.stop_words  # spaCyのモデルのストップワードをロード\n",
    "custom_stopwords = load_stopwords(custom_stopwords_path)  # ユーザー定義のストップワードを読み込む\n",
    "\n",
    "if use_spacy_stopwords:\n",
    "    combined_stopwords = spacy_stopwords | custom_stopwords  # spacyのデフォルトストップワードとユーザー定義のストップワードの和集合を取る\n",
    "    print(\"spacyのデフォルトストップワードを使用します。\")\n",
    "else:\n",
    "    combined_stopwords = custom_stopwords  # ユーザー定義のストップワードのみを使用\n",
    "    print(\"spacyのデフォルトストップワードは使用しません。\")\n",
    "\n",
    "for doc in filtered_docs.values():  # 読み込んだ文書に対して処理を行う\n",
    "    for sent in doc.sents:  # 各文に対して処理を行う\n",
    "        words = extract_words(sent, include_pos, combined_stopwords)\n",
    "        all_sents.append(' '.join(words)) # 前処理後のトークンを空白区切りで連結し、リストに格納\n",
    "\n",
    "print(all_sents[:10])  # 前処理後の文を10件表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **共起行列の作成**\n",
    "## ユーザーが設定するパラメーター"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エッジの重みを計算する方法を選択(cooccurrence, jaccard, dice, simpson)\n",
    "weight_mode = 'cooccurrence' \n",
    "\n",
    "# 上限と下限の出現回数を設定(以上/以下)\n",
    "min_freq = 2\n",
    "max_freq = 100000\n",
    "\n",
    "# 文の最小出現回数を設定(jaccard, dice, simpsonの場合のみ有効)\n",
    "min_sentence_count = 1\n",
    "\n",
    "# トークンの最小長を設定(N文字以上を対象にする、1文字の単語を除外する場合は2以上に設定)\n",
    "token_length = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 共起行列作成の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_cooccurrence(sents, min_freq, max_freq, token_length):\n",
    "    \"\"\"\n",
    "    共起行列を生成する関数\n",
    "\n",
    "    Args:\n",
    "        sents (list): 文のリスト\n",
    "        min_freq (int): 出現回数の下限\n",
    "        max_freq (int): 出現回数の上限\n",
    "        token_length (int): トークンの最小長\n",
    "\n",
    "    Returns:\n",
    "        tuple: 以下の要素を持つタプル\n",
    "            - words (list): 語彙のリスト\n",
    "            - token_freq_dict (dict): 語彙の出現回数の辞書\n",
    "            - Xc (scipy.sparse.csr_matrix): 共起行列\n",
    "    \"\"\"\n",
    "    token_pattern = f'\\\\b\\\\w{{{token_length},}}\\\\b'  # token_length文字以上の単語を抽出する正規表現パターン\n",
    "    vectorizer = CountVectorizer(token_pattern=token_pattern) # CountVectorizerの初期化\n",
    "    X = vectorizer.fit_transform(sents) # 前処理後の文を行列に変換\n",
    "    \n",
    "    # 語彙と出現回数を取得\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    frequencies = X.sum(axis=0).A1\n",
    "    \n",
    "    # 出現回数に基づいて語彙をフィルタリング\n",
    "    mask = (min_freq <= frequencies) & (frequencies <= max_freq) # 出現回数がmin_freq以上max_freq以下の語彙を選択\n",
    "    filtered_vocab = [feat for feat, m in zip(feature_names, mask) if m] # フィルタリングされた語彙を取得\n",
    "    \n",
    "    # フィルタリングされた語彙を使って再度CountVectorizerを適用\n",
    "    filtered_vectorizer = CountVectorizer(vocabulary=filtered_vocab, token_pattern=token_pattern) # フィルタリングされた語彙を使ってCountVectorizerを初期化\n",
    "    filtered_X = filtered_vectorizer.fit_transform(sents) # フィルタリングされた語彙を使って文を行列に変換\n",
    "    \n",
    "    # 共起行列の生成\n",
    "    words = filtered_vectorizer.get_feature_names_out()\n",
    "    token_freq = filtered_X.sum(axis=0).A1\n",
    "    token_freq_dict = dict(zip(words, token_freq))\n",
    "    filtered_X[filtered_X > 0] = 1  # 各要素が1以上の場合は1に変換\n",
    "    Xc = (filtered_X.T * filtered_X)  # 共起行列の生成\n",
    "    \n",
    "    return words, token_freq_dict, Xc\n",
    "\n",
    "def cooccurrence_weights(words, Xc):\n",
    "    \"\"\"\n",
    "    共起行列から単語間の重みを計算する関数\n",
    "\n",
    "    Args:\n",
    "        words (list): 語彙のリスト\n",
    "        Xc (scipy.sparse.csr_matrix): 共起行列\n",
    "\n",
    "    Returns:\n",
    "        list: (単語1, 単語2, 重み) のタプルのリスト\n",
    "    \"\"\"\n",
    "    Xc.setdiag(0) # 対角成分を0にする\n",
    "    Xc_max = Xc.max() # 共起回数の最大値を取得\n",
    "    weights = [(words[i], words[j], Xc[i,j] / Xc_max) for i, j in zip(*Xc.nonzero()) if i < j] # 上三角行列のみ取得し、正規化を行う\n",
    "    return weights\n",
    "\n",
    "def cooccurrence_similarity(words, sents, Xc, mode, min_sentence_count=1):\n",
    "    \"\"\"\n",
    "    共起行列から単語間の類似度を計算する関数\n",
    "\n",
    "    Args:\n",
    "        words (list): 語彙のリスト\n",
    "        sents (list): 文のリスト\n",
    "        Xc (scipy.sparse.csr_matrix): 共起行列\n",
    "        mode (str): 類似度の計算方法 ('jaccard', 'dice', 'simpson')\n",
    "        min_sentence_count (int): 単語が出現する最小文数の閾値 . Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        tuple: 以下の要素を持つタプル\n",
    "            - weights (list): (単語1, 単語2, 類似度) のタプルのリスト\n",
    "            - csv_data (list): CSV出力用のデータのリスト\n",
    "    \"\"\"\n",
    "    word_to_sents = {word: set() for word in words}\n",
    "    for i, sent in enumerate(sents):\n",
    "        for word in sent.split():\n",
    "            if word in word_to_sents:\n",
    "                word_to_sents[word].add(i)\n",
    "\n",
    "    weights = []\n",
    "    csv_data = []\n",
    "    for i, j in zip(*Xc.nonzero()):\n",
    "        if i < j: \n",
    "            set_i = word_to_sents[words[i]]\n",
    "            set_j = word_to_sents[words[j]]\n",
    "            count1 = len(set_i)\n",
    "            count2 = len(set_j)\n",
    "\n",
    "            if count1 > min_sentence_count and count2 > min_sentence_count:\n",
    "                intersection = len(set_i & set_j)\n",
    "\n",
    "                if mode == 'jaccard':\n",
    "                    union = len(set_i | set_j)\n",
    "                    if union > 0:\n",
    "                        jaccard = intersection / union\n",
    "                        csv_data.append([words[i], words[j], intersection, count1, count2, union, jaccard])\n",
    "                        weights.append((words[i], words[j], jaccard))\n",
    "                elif mode == 'dice':\n",
    "                    dice = (2 * intersection) / (count1 + count2)\n",
    "                    csv_data.append([words[i], words[j], intersection, count1, count2, dice])\n",
    "                    weights.append((words[i], words[j], dice))\n",
    "                elif mode == 'simpson':\n",
    "                    simpson = intersection / min(count1, count2)\n",
    "                    csv_data.append([words[i], words[j], intersection, count1, count2, simpson])\n",
    "                    weights.append((words[i], words[j], simpson))\n",
    "\n",
    "    return weights, csv_data\n",
    "\n",
    "def calculate_weights(words, all_sents, Xc, weight_mode, min_sentence_count=1):\n",
    "    \"\"\"\n",
    "    エッジの重み計算の方法に応じて関数を選択し、重みを計算する関数\n",
    "\n",
    "    Args:\n",
    "        words (list): 語彙のリスト\n",
    "        all_sents (list): 全ての文のリスト\n",
    "        Xc (scipy.sparse.csr_matrix): 共起行列\n",
    "        weight_mode (str): 重み計算の方法 ('cooccurrence', 'jaccard', 'dice', 'simpson')\n",
    "        min_sentence_count (int): 単語が出現する最小文数の閾値 . Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        list: (単語1, 単語2, 重み) のタプルのリスト\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: 不明な重み計算方法が指定された場合\n",
    "    \"\"\"\n",
    "    if weight_mode == 'cooccurrence':\n",
    "        weights = cooccurrence_weights(words, Xc)\n",
    "        csv_header = ['word1', 'word2', 'cooccurrence']\n",
    "    elif weight_mode in ['jaccard', 'dice', 'simpson']:\n",
    "        weights, csv_data = cooccurrence_similarity(words, all_sents, Xc, weight_mode, min_sentence_count)\n",
    "        if weight_mode == 'jaccard':\n",
    "            csv_header = ['word1', 'word2', 'intersection', 'count1', 'count2', 'union', 'jaccard']\n",
    "        elif weight_mode == 'dice':\n",
    "            csv_header = ['word1', 'word2', 'intersection', 'count1', 'count2', 'dice']\n",
    "        else:  # simpson\n",
    "            csv_header = ['word1', 'word2', 'intersection', 'count1', 'count2', 'simpson']\n",
    "    else:\n",
    "        raise ValueError(f\"不明な重み計算方法: {weight_mode}\")\n",
    "    \n",
    "    weights.sort(key=lambda x: x[2], reverse=True)  # 重みでソート\n",
    "    weights = weights[:10000]  # 上位10000件のみを取得\n",
    "    \n",
    "    if weight_mode != 'cooccurrence':\n",
    "        csv_data.sort(key=lambda x: x[-1], reverse=True)  # 重みでソート\n",
    "        csv_data = csv_data[:10000]  # 上位10000件のみを取得\n",
    "        \n",
    "        with open(f\"{weight_mode}_edge.csv\", 'w', encoding='utf-8', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(csv_header)\n",
    "            writer.writerows(csv_data)\n",
    "    else:\n",
    "        with open('cooccurrence_edge.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(csv_header)\n",
    "            writer.writerows(weights)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "# 共起行列の作成\n",
    "words, token_freq_dict, Xc = count_cooccurrence(all_sents, min_freq, max_freq, token_length)\n",
    "# 共起行列のトークン情報(ストップワードや出現回数のフィルタリング後の語彙)\n",
    "with open(\"filtered_tokens.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for token in words:\n",
    "        f.write(token + \"\\n\")\n",
    "\n",
    "# エッジの重みを計算\n",
    "weights = calculate_weights(words, all_sents, Xc, weight_mode, min_sentence_count)\n",
    "\n",
    "# 情報の表示\n",
    "print(f\"共起行列のサイズ: {Xc.shape}\") \n",
    "print(f\"選択された重み計算方法: {weight_mode}\")\n",
    "print(f\"エッジ数: {len(weights)}\")\n",
    "print(weights[:10])  # 上位10件のエッジを表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **共起ネットワークの作成**\n",
    "## ユーザーが設定するパラメーター"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エッジの重みの範囲を指定(以上/以下)\n",
    "weight_lower_cutoff = 0.02\n",
    "weight_upper_cutoff = 1\n",
    "\n",
    "# 出力するエッジの数を指定（None の場合は全て出力）\n",
    "num_edges_to_output = 1000\n",
    "\n",
    "# カットオフする独立したエッジの数を指定(0の場合は全て出力)\n",
    "max_degree = 1\n",
    "\n",
    "# ノードとエッジのサイズを調整するための係数\n",
    "node_multiplier = 50\n",
    "edge_multiplier = 20\n",
    "\n",
    "# ノードの重みを計算する方法を選択('centrality' または 'frequency')\n",
    "node_weight_mode = 'centrality'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実行セル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_edges(weights, lower_cutoff, upper_cutoff, num_edges=None):\n",
    "    \"\"\"\n",
    "    エッジの重みの範囲に基づいてエッジをフィルタリングする関数\n",
    "\n",
    "    Args:\n",
    "        weights (list): (単語1, 単語2, 重み) のタプルのリスト\n",
    "        lower_cutoff (float): 重みの下限値\n",
    "        upper_cutoff (float): 重みの上限値\n",
    "        num_edges (int, optional): 出力するエッジの数. Noneの場合は全て出力. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list: フィルタリングされたエッジのリスト\n",
    "    \"\"\"\n",
    "    filtered_weights = [edge for edge in weights if lower_cutoff <= edge[2] <= upper_cutoff] # 指定された範囲のエッジを取得(下限以上,上限以下)\n",
    "    sorted_weights = sorted(filtered_weights, key=lambda x: x[2], reverse=True) # 重みでソート\n",
    "    \n",
    "    # エッジ数の指定がある場合は上位のエッジのみを返す\n",
    "    if num_edges is None or num_edges >= len(sorted_weights): \n",
    "        return sorted_weights\n",
    "    else:\n",
    "        return sorted_weights[:num_edges]\n",
    "\n",
    "def remove_isolated_edges(G, max_degree):\n",
    "    \"\"\"\n",
    "    グラフから独立したエッジを削除する関数\n",
    "\n",
    "    Args:\n",
    "        G (networkx.Graph): NetworkXのグラフオブジェクト\n",
    "        max_degree (int): カットオフする独立したエッジの最大次数\n",
    "\n",
    "    Returns:\n",
    "        networkx.Graph: 独立したエッジを削除したグラフ\n",
    "    \"\"\"\n",
    "    edges_to_remove = [] # 削除するエッジのリスト\n",
    "    for u, v in G.edges(): # 全てのエッジに対して処理を行う\n",
    "        if G.degree(u) <= max_degree and G.degree(v) <= max_degree: # 両端のノードの次数がmax_degree以下の場合\n",
    "            edges_to_remove.append((u, v)) # エッジを削除するリストに追加\n",
    "    G.remove_edges_from(edges_to_remove) # エッジの削除\n",
    "    \n",
    "    # 孤立したノードの削除\n",
    "    isolated_nodes = [node for node in G.nodes() if G.degree(node) == 0] # 次数が0のノードを取得\n",
    "    G.remove_nodes_from(isolated_nodes) # 孤立したノードを削除\n",
    "    \n",
    "    return G\n",
    "\n",
    "def create_network_with_communities(words, weights, max_degree, node_weight_mode):\n",
    "    \"\"\"\n",
    "    共起ネットワークを作成し、コミュニティを検出する関数\n",
    "\n",
    "    Args:\n",
    "        words (list): 語彙のリスト\n",
    "        weights (list): (単語1, 単語2, 重み) のタプルのリスト\n",
    "        max_degree (int): カットオフする独立したエッジの最大次数\n",
    "        node_weight_mode (str): ノードの重み計算方法 ('centrality' または 'frequency')\n",
    "\n",
    "    Returns:\n",
    "        networkx.Graph: コミュニティ情報を含む共起ネットワーク\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    filtered_weights = filter_edges(weights, weight_lower_cutoff, weight_upper_cutoff, num_edges_to_output)\n",
    "    G.add_weighted_edges_from(filtered_weights)\n",
    "\n",
    "    # エラー処理\n",
    "    if not G.nodes() or not G.edges():\n",
    "        raise ValueError(\"ネットワークが空です。エッジの重みの範囲を調整してください。\")\n",
    "\n",
    "    # 独立したエッジの削除\n",
    "    G = remove_isolated_edges(G, max_degree)\n",
    "\n",
    "    # コミュニティ検出\n",
    "    communities = community.greedy_modularity_communities(G)\n",
    "    community_dict = {node: cid for cid, com in enumerate(communities) for node in com}\n",
    "\n",
    "    # **ノードの重み計算**\n",
    "    if node_weight_mode == 'centrality':\n",
    "        # 中心性に基づく重み計算\n",
    "        centrality = nx.degree_centrality(G)\n",
    "        max_centrality = max(centrality.values(), default=1)\n",
    "        normalized_weights = {node: centrality[node] / max_centrality for node in G.nodes()}\n",
    "    elif node_weight_mode == 'frequency':\n",
    "        # 出現頻度に基づく重み計算\n",
    "        node_freq = {node: token_freq_dict.get(node, 0) for node in G.nodes()}\n",
    "        max_freq = max(node_freq.values(), default=1)\n",
    "        normalized_weights = {node: node_freq[node] / max_freq for node in G.nodes()}\n",
    "    else:\n",
    "        raise ValueError(f\"不明なノード重み計算方法: {node_weight_mode}\")\n",
    "\n",
    "    # ノード属性の設定\n",
    "    for node in G.nodes():\n",
    "        G.nodes[node]['weight'] = normalized_weights[node]\n",
    "        G.nodes[node]['community'] = community_dict.get(node, 0)\n",
    "\n",
    "    return G\n",
    "\n",
    "def nx2pyvis_G(G, node_multiplier, edge_multiplier, min_size=0.01):\n",
    "    \"\"\"\n",
    "    NetworkXのグラフをPyvisのグラフに変換する関数\n",
    "\n",
    "    Args:\n",
    "        G (networkx.Graph): NetworkXのグラフオブジェクト\n",
    "        node_multiplier (float): ノードサイズの調整係数\n",
    "        edge_multiplier (float): エッジサイズの調整係数\n",
    "        min_size (float, optional): ノードの最小サイズ. Defaults to 0.01.\n",
    "\n",
    "    Returns:\n",
    "        pyvis.network.Network: Pyvisのグラフオブジェクト\n",
    "    \"\"\"\n",
    "    # Pyvisのグラフオブジェクトの初期化\n",
    "    pyvis_G = Network(height=\"850px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\", filter_menu=True) \n",
    "    \n",
    "    # ノードに情報を追加(NetworkXのノード属性をPyvisのノード属性に変換)\n",
    "    for node, attrs in G.nodes(data=True): # 全てのノードに対して処理を行う(ノード名, 属性の辞書)\n",
    "        size = max(node_multiplier * attrs.get('weight', 1), min_size) # ノードのサイズを計算\n",
    "        pyvis_G.add_node(node, title=node, size=size, group=attrs.get('community', 0)) # ノードを追加\n",
    "    \n",
    "    # エッジに情報を追加(NetworkXのエッジ属性をPyvisのエッジ属性に変換)\n",
    "    for node1, node2, attrs in G.edges(data=True):\n",
    "        weight = attrs.get('weight', 1) \n",
    "        title = f\"{node1} - {node2}\\nWeight: {weight:.2f}\" \n",
    "        pyvis_G.add_edge(node1, node2, title=title, width=edge_multiplier * weight)\n",
    "    \n",
    "    pyvis_G.force_atlas_2based()\n",
    "    \n",
    "    # 隣接ノードの情報を取得\n",
    "    neighbor_map = pyvis_G.get_adj_list()\n",
    "    \n",
    "    # 各ノードの情報を更新\n",
    "    for node in pyvis_G.nodes:\n",
    "        weight_info = G.nodes[node['id']].get('weight', 1)\n",
    "        title_text = f\"{node['id']}\\nWeight: {weight_info:.2f}\\nCommunity: {node['group']}\\nNeighbors:\\n\"\n",
    "        title_text += \"\\n\".join(neighbor_map[node['id']])\n",
    "        node['title'] = title_text\n",
    "    \n",
    "    pyvis_G.show_buttons(filter_=['physics', 'nodes']) # 設定ボタンの表示\n",
    "    \n",
    "    return pyvis_G\n",
    "\n",
    "# ネットワークの作成\n",
    "G = create_network_with_communities(words, weights, max_degree, node_weight_mode)\n",
    "\n",
    "# Pyvisでの可視化\n",
    "pyvis_G = nx2pyvis_G(G, node_multiplier, edge_multiplier)\n",
    "pyvis_G.save_graph(\"network.html\")\n",
    "\n",
    "# ネットワークのノードとエッジ数を表示\n",
    "print(f\"ノード数: {G.number_of_nodes()}\")\n",
    "print(f\"エッジ数: {G.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ネットワーク情報の出力(CSVファイル)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エッジ情報のCSV出力\n",
    "with open(\"network_edges.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Source\", \"Target\", \"Weight\"])\n",
    "    for u, v, attrs in G.edges(data=True):\n",
    "        writer.writerow([u, v, attrs.get(\"weight\", 1)])\n",
    "\n",
    "# ノード情報のCSV出力\n",
    "with open(\"network_nodes.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Community\", \"Node\", \"Frequency\", \"Degree\", \"Weight\"])\n",
    "    for node, attrs in G.nodes(data=True):\n",
    "        writer.writerow([attrs.get(\"community\", 0), node, token_freq_dict.get(node, 0), G.degree(node), attrs.get(\"weight\", 1)])\n",
    "\n",
    "print(\"ネットワークのデータをCSVファイルに保存しました。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kasou",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
