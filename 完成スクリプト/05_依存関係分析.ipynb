{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 依存関係分析プログラム"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ライブラリのインポート（最初に実行）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.matcher import DependencyMatcher\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **解析データの読み込み**\n",
    "## ユーザーが設定するパラメーター"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本設定\n",
    "main_directory = \"processed_data\"  # 親ディレクトリへの相対パス\n",
    "subdirectories = [\"sub1\"]  # 読み込む子ディレクトリ名のリスト\n",
    "nlp_model = \"en_core_web_sm\"  # 解析に利用したモデル名\n",
    "\n",
    "# 特定のファイルのみを読み込む場合は以下のリストにtxtファイル名を追加(空リストの場合は全ファイルを読み込む)\n",
    "selected_file_names = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解析データ読み込みの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCyの言語モデルをロード\n",
    "nlp = spacy.load(nlp_model)\n",
    "\n",
    "# 解析結果を格納する辞書の初期化\n",
    "docs_dict = {}\n",
    "# フィルタリングされたDocオブジェクトを格納する辞書の初期化\n",
    "filtered_docs = {}\n",
    "\n",
    "# 指定されたサブディレクトリ内のファイルを読み込む\n",
    "for subdir in subdirectories:\n",
    "    directory = os.path.join(main_directory, subdir)\n",
    "    \n",
    "    # サブディレクトリ内のファイルを読み込む\n",
    "    for filename in os.listdir(directory): \n",
    "        if filename.endswith(\".spacy\"):  # .spacyファイルならば\n",
    "            spacy_path = os.path.join(directory, filename)\n",
    "            txt_filename = filename.replace(\".spacy\", \".txt\")  # .spaCyファイルに対応するテキストファイル名を作成\n",
    "            txt_path = os.path.join(directory, txt_filename)  # .spacyファイルに対応するテキストファイルのパス\n",
    "            \n",
    "            # .spacyファイルからDocオブジェクトを読み込む\n",
    "            doc_bin = DocBin().from_disk(spacy_path)  # DocBinオブジェクトを読み込む\n",
    "            docs = list(doc_bin.get_docs(nlp.vocab))  # Docオブジェクトをリストに格納\n",
    "            \n",
    "            # 対応する.txtファイルからファイル名を読み込む\n",
    "            if os.path.exists(txt_path):\n",
    "                with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "                    file_names = [line.strip() for line in f.readlines()]  # ファイル名をリストに格納\n",
    "                \n",
    "                # Docオブジェクトとファイル名の対応を辞書に格納\n",
    "                for doc, fname in zip(docs, file_names):\n",
    "                    docs_dict[fname] = doc\n",
    "            else:\n",
    "                print(f\"警告: {txt_filename} に対応するテキストファイルが存在しません。\")\n",
    "\n",
    "# ユーザーがファイル名を指定した場合、それに対応するDocオブジェクトをフィルタリング\n",
    "if selected_file_names:\n",
    "    missing_files = []  # 存在しないファイル名を格納するリスト\n",
    "    for fname in selected_file_names:\n",
    "        if fname in docs_dict:  # 指定されたファイル名が読み込んだデータに存在する場合\n",
    "            filtered_docs[fname] = docs_dict[fname]\n",
    "        else:\n",
    "            missing_files.append(fname)\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"警告: 次の指定されたファイルは存在しません: {', '.join(missing_files)}\")\n",
    "    if filtered_docs:\n",
    "        print(\"指定された一部のデータが読み込まれました。\")\n",
    "    else:\n",
    "        print(\"指定されたファイルに対応するデータが見つかりませんでした。\")\n",
    "else:\n",
    "    filtered_docs = docs_dict\n",
    "    print(\"全てのデータが読み込まれました。\")\n",
    "print(f\"読み込まれたデータ数: {len(filtered_docs)}\")\n",
    "print(f\"読み込まれたファイル名: {list(filtered_docs.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **依存関係マッチャ―**\n",
    "## ユーザーが設定するパラメーター"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"RIGHT_ID\": \"verb\", \"RIGHT_ATTRS\": {\"POS\": \"VERB\"}},\n",
    "    {\"LEFT_ID\": \"verb\", \"REL_OP\": \">\", \"RIGHT_ID\": \"subject\", \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\", \"LOWER\": {\"IN\": [\"bank\", \"fed\", \"government\"]}}},\n",
    "    {\"LEFT_ID\": \"verb\", \"REL_OP\": \">\", \"RIGHT_ID\": \"object\", \"RIGHT_ATTRS\": {\"DEP\": \"dobj\", \"LOWER\": {\"IN\": [\"rates\", \"policy\", \"markets\"]}}}\n",
    "]\n",
    "\n",
    "# 出力するファイル名\n",
    "output_file = \"dep_matched_sentences.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Matcherの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency Matcherのインスタンスを作成\n",
    "matcher = DependencyMatcher(nlp.vocab)\n",
    "# パターンをMatcherに追加\n",
    "matcher.add(\"PATTERN\", [pattern])\n",
    "\n",
    "# マッチした文を格納するためのセット（重複を避ける）\n",
    "matched_sentences_set = set()\n",
    "\n",
    "# マッチした文を探す\n",
    "for doc in filtered_docs.values():\n",
    "    matches = matcher(doc)\n",
    "    for match_id, token_ids in matches:\n",
    "        for token_id in token_ids:\n",
    "            sentence = doc[token_id].sent\n",
    "            # 文をセットに追加（重複を避ける）\n",
    "            matched_sentences_set.add(sentence.text)\n",
    "\n",
    "# マッチした文をテキストファイルに出力\n",
    "def save_matched_sentences(matched_sentences_set, output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for sentence in matched_sentences_set:\n",
    "            f.write(f\"{sentence}\\n\")\n",
    "\n",
    "# マッチした文の総数をプリント\n",
    "if matched_sentences_set:\n",
    "    print(f\"検索結果: {len(matched_sentences_set)} 件の結果が見つかりました。\")\n",
    "    save_matched_sentences(matched_sentences_set, output_file)\n",
    "    print(f\"検索結果を {output_file} に保存しました。\")\n",
    "else:\n",
    "    print(\"検索結果が見つかりませんでした。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **依存関係指定の用例抽出**\n",
    "## ユーザーが設定するパラメーター"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽出する特徴語と依存関係のタイプを指定\n",
    "feature_words = [\"economy\", \"inflation\", \"growth\", \"market\", \"stock\"]\n",
    "dependency_types = ['dobj', 'pobj', 'nsubj', 'xcomp', 'nsubjpass', 'agent', 'acomp', 'compound', 'prep']\n",
    "\n",
    "# 出力ディレクトリの設定\n",
    "output_directory = \"dependency_output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用例抽出の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(docs, feature_words, output_dir, dependency_types):\n",
    "    \"\"\"\n",
    "    指定された依存関係を持つ文を抽出し、特徴語ごとにテキストファイルに出力する関数。\n",
    "\n",
    "    Args:\n",
    "        docs (dict): Docオブジェクトを格納した辞書。キーはファイル名、値はDocオブジェクト。\n",
    "        feature_words (list): 特徴語のリスト。\n",
    "        output_dir (str): 出力先のディレクトリ名。\n",
    "        dependency_types (list): 抽出する依存関係のリスト。\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for word in feature_words:\n",
    "        output_path = os.path.join(output_dir, f\"{word}.txt\")\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for dep_type in dependency_types:\n",
    "                f.write(f\"● {word.upper()} ({dep_type.upper()})\\n\")\n",
    "                sentence_count = 1\n",
    "                for doc_name, doc in docs.items():\n",
    "                    for sent in doc.sents:\n",
    "                        for token in sent:\n",
    "                            if token.lemma_.lower() == word.lower() and token.dep_ == dep_type:\n",
    "                                f.write(f\" {sentence_count}. {sent.text}\\n\")\n",
    "                                sentence_count += 1\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "# 文の抽出と出力\n",
    "extract_sentences(filtered_docs, feature_words, output_directory, dependency_types)\n",
    "# 出力の完了を報告\n",
    "print(f\"文の抽出が完了しました。出力先: {output_directory}\")\n",
    "print(f\"抽出された特徴語: {feature_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **依存関係の可視化**\n",
    "## 依存関係を表示（入力文を解析）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解析したい文を入力\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
    "\n",
    "# 文をDocオブジェクトに変換\n",
    "nlp = spacy.load(nlp_model)\n",
    "doc = nlp(text)\n",
    "options = {\"compact\": True, \"bg\": \"#09a3d5\",\"color\": \"white\", \"font\": \"Source Sans Pro\", \"distance\": 110}\n",
    "\n",
    "# 依存関係を表示\n",
    "displacy.render(doc, style=\"dep\", jupyter=True, options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 依存関係を表示（元の解析済みデータから依存関係を取得し表示）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検索キーワードを定義（このキーワードを含む文を検索します）\n",
    "search_keyword = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "\n",
    "# マッチした文の依存関係を表示する関数\n",
    "def display_dependency(doc):\n",
    "    options = {\"compact\": True, \"bg\": \"#09a3d5\",\"color\": \"white\", \"font\": \"Source Sans Pro\", \"distance\": 110}\n",
    "    displacy.render(doc, style=\"dep\", jupyter=True, options=options)\n",
    "\n",
    "# マッチした文を探す\n",
    "for file_name, doc in filtered_docs.items():\n",
    "    # doc内の各文（sent）に対してループ\n",
    "    for sent in doc.sents:\n",
    "        if search_keyword in sent.text:\n",
    "            print(f\"ファイル '{file_name}' から見つかった文: {sent.text}\")\n",
    "            display_dependency(sent)\n",
    "            break  # 1つ見つかった時点でループを抜ける\n",
    "    # 見つからなかった場合の処理\n",
    "    else:\n",
    "        print(f\"ファイル '{file_name}' に '{search_keyword}' を含む文は見つかりませんでした。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
