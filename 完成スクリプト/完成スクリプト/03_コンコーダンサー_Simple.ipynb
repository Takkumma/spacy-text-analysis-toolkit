{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# コンコーダンサー(Simple)\n",
    "\n",
    "このスクリプトは、spaCyライブラリを使用して事前に解析されたテキストデータに対してコンコーダンス検索を行い、検索結果をHTMLファイルやテキストファイルに出力する。また、検索結果をソートする機能も備える。\n",
    "\n",
    "## 使用方法\n",
    "\n",
    "### 解析データの読み込み\n",
    "\n",
    "1. 解析済みのテキストデータ（.spacyファイル）とそれに対応するテキストファイル（.txt）を用意する。\n",
    "2. スクリプト内の「ユーザーが設定するパラメーター」セクションで以下の変数を設定する。\n",
    "   - `main_directory`: 親ディレクトリへの相対パス\n",
    "   - `subdirectories`: 読み込む子ディレクトリ名のリスト\n",
    "   - `nlp_model`: 解析に利用したモデル名\n",
    "   - `selected_file_names`: 特定のファイルのみを読み込む場合はここにtxtファイル名を追加（空リストの場合は全ファイルを読み込む）\n",
    "\n",
    "\n",
    "### コンコーダンサーの実行\n",
    "\n",
    "1. スクリプト内の「ユーザーが設定するパラメーター」セクションで以下の変数を設定する。\n",
    "   - `search_type`: 検索タイプ（'word', 'lemma', 'phrase', 'regex'）\n",
    "   - `input_keyword`: 検索するキーワード\n",
    "   - `part_of_speech`: 品詞（word, lemma検索で使用可能, 'NOUN', 'VERB' 等、または None）\n",
    "   - `window_size_left`, `window_size_right`: KWIC表示の左右のウィンドウサイズ\n",
    "   - `language_direction`: 言語の方向（\"ltr\" または \"rtl\"）\n",
    "   - `output_html_file`: 出力するHTMLファイル名\n",
    "2. スクリプトを実行すると、指定した設定に基づいて検索が行われ、結果が出力されます。\n",
    "\n",
    "\n",
    "## 追加機能\n",
    "\n",
    "このスクリプトには、以下のような追加機能が含まれる。\n",
    "\n",
    "- ソート機能: 検索結果をキーワードや周辺の単語を基準にソートできる。\n",
    "- テキストファイル出力機能: 検索結果をテキストファイルに出力できる。Window単位での出力と文単位での出力が可能。\n",
    "- CSV形式での出力: 検索結果をCSV形式で出力できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ライブラリのインポート(最初に実行)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **解析データの読み込み**\n",
    "## ユーザーが設定するパラメーター"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本設定\n",
    "main_directory = \"processed_data\"  # 親ディレクトリへの相対パス\n",
    "subdirectories = [\"sub1\"]  # 読み込む子ディレクトリ名のリスト\n",
    "nlp_model = \"en_core_web_sm\"  # 解析に利用したモデル名\n",
    "\n",
    "# 特定のファイルのみを読み込む場合は以下のリストにtxtファイル名を追加(空リストの場合は全ファイルを読み込む)\n",
    "selected_file_names = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解析データ読み込みの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCyの言語モデルをロード\n",
    "nlp = spacy.load(nlp_model)\n",
    "\n",
    "# 解析結果を格納する辞書の初期化\n",
    "docs_dict = {}\n",
    "# フィルタリングされたDocオブジェクトを格納する辞書の初期化\n",
    "filtered_docs = {}\n",
    "\n",
    "# 指定されたサブディレクトリ内のファイルを読み込む\n",
    "for subdir in subdirectories:\n",
    "    directory = os.path.join(main_directory, subdir)\n",
    "    \n",
    "    # サブディレクトリ内のファイルを読み込む\n",
    "    for filename in os.listdir(directory): \n",
    "        if filename.endswith(\".spacy\"):  # .spacyファイルならば\n",
    "            spacy_path = os.path.join(directory, filename)\n",
    "            txt_filename = filename.replace(\".spacy\", \".txt\")  # .spaCyファイルに対応するテキストファイル名を作成\n",
    "            txt_path = os.path.join(directory, txt_filename)  # .spacyファイルに対応するテキストファイルのパス\n",
    "            \n",
    "            # .spacyファイルからDocオブジェクトを読み込む\n",
    "            doc_bin = DocBin().from_disk(spacy_path)  # DocBinオブジェクトを読み込む\n",
    "            docs = list(doc_bin.get_docs(nlp.vocab))  # Docオブジェクトをリストに格納\n",
    "            \n",
    "            # 対応する.txtファイルからファイル名を読み込む\n",
    "            if os.path.exists(txt_path):\n",
    "                with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "                    file_names = [line.strip() for line in f.readlines()]  # ファイル名をリストに格納\n",
    "                \n",
    "                # Docオブジェクトとファイル名の対応を辞書に格納\n",
    "                for doc, fname in zip(docs, file_names):\n",
    "                    docs_dict[fname] = doc\n",
    "            else:\n",
    "                print(f\"警告: {txt_filename} に対応するテキストファイルが存在しません。\")\n",
    "\n",
    "# ユーザーがファイル名を指定した場合、それに対応するDocオブジェクトをフィルタリング\n",
    "if selected_file_names:\n",
    "    missing_files = []  # 存在しないファイル名を格納するリスト\n",
    "    for fname in selected_file_names:\n",
    "        if fname in docs_dict:  # 指定されたファイル名が読み込んだデータに存在する場合\n",
    "            filtered_docs[fname] = docs_dict[fname]\n",
    "        else:\n",
    "            missing_files.append(fname)\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"警告: 次の指定されたファイルは存在しません: {', '.join(missing_files)}\")\n",
    "    if filtered_docs:\n",
    "        print(\"指定された一部のデータが読み込まれました。\")\n",
    "    else:\n",
    "        print(\"指定されたファイルに対応するデータが見つかりませんでした。\")\n",
    "else:\n",
    "    filtered_docs = docs_dict\n",
    "    print(\"全てのデータが読み込まれました。\")\n",
    "print(f\"読み込まれたデータ数: {len(filtered_docs)}\")\n",
    "print(f\"読み込まれたファイル名: {list(filtered_docs.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **コンコーダンサー**\n",
    "## ユーザーが設定するパラメータ―"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本設定\n",
    "search_type = 'phrase'    # 検索タイプ（'word', 'lemma', 'phrase', 'regex'）\n",
    "input_keyword = \"hello world\"  # 検索するキーワード\n",
    "\n",
    "# 追加設定\n",
    "part_of_speech = None        # 品詞（word, lemma検索で使用可能, 'NOUN', 'VERB' 等、または None）\n",
    "\n",
    "# window sizeを指定(左右に何単語分表示するか)\n",
    "window_size_left = 15       # KWIC表示の左側のウィンドウサイズ\n",
    "window_size_right = 15      # KWIC表示の右側のウィンドウサイズ\n",
    "\n",
    "# 言語の設定\n",
    "language_direction = \"ltr\"  # \"ltr\" (left-to-right) または \"rtl\" (right-to-left)\n",
    "\n",
    "# 出力するHTMLファイル名\n",
    "output_html_file = \"kwic_results.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 検索の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_word(doc, keyword, part_of_speech, window_size_left, window_size_right, keyword_positions, file_name):\n",
    "    \"\"\"\n",
    "    単語レベルでキーワードを検索し、KWIC結果を返す関数。\n",
    "\n",
    "    Args:\n",
    "        doc (Doc): spaCyのDocオブジェクト。\n",
    "        keyword (str): 検索するキーワード。\n",
    "        part_of_speech (str or None): 検索する品詞。Noneの場合は品詞を指定しない。\n",
    "        window_size_left (int): 左側のコンテキストのウィンドウサイズ。\n",
    "        window_size_right (int): 右側のコンテキストのウィンドウサイズ。\n",
    "        keyword_positions (list): キーワードの位置情報とファイル名を格納するリスト。\n",
    "        file_name (str): 検索対象のファイル名。\n",
    "\n",
    "    Returns:\n",
    "        list: KWIC結果のリスト。[(left_context, keyword, right_context), ...]\n",
    "    \"\"\"\n",
    "    results = []  # 結果を格納するリスト\n",
    "    for token in doc:\n",
    "        if part_of_speech is None or token.pos_ == part_of_speech:  # 品詞が指定されていないか、指定された品詞の場合\n",
    "            if token.text.lower() == keyword.lower():  # キーワードが一致する場合(小文字で比較)\n",
    "                # 左右のコンテキストをトークンのリストとして取得\n",
    "                left_context = [t.text_with_ws for t in doc[max(0, token.i - window_size_left):token.i]]  # 左側のコンテキスト(maxを使用して負のインデックスを防ぐ)\n",
    "                right_context = [t.text_with_ws for t in doc[token.i + 1:token.i + 1 + window_size_right]] # 右側のコンテキスト\n",
    "\n",
    "                # 結果をリストに追加\n",
    "                results.append((left_context, [token.text], right_context, file_name))\n",
    "                keyword_positions.append((token.i, file_name))  # キーワードの位置情報とファイル名を追加\n",
    "    return results\n",
    "\n",
    "def search_lemma(doc, keyword, part_of_speech, window_size_left, window_size_right, keyword_positions, file_name):\n",
    "    \"\"\"\n",
    "    レンマレベルでキーワードを検索し、KWIC結果を返す関数。\n",
    "\n",
    "    Args:\n",
    "        doc (Doc): spaCyのDocオブジェクト。\n",
    "        keyword (str): 検索するキーワード。\n",
    "        part_of_speech (str or None): 検索する品詞。Noneの場合は品詞を指定しない。\n",
    "        window_size_left (int): 左側のコンテキストのウィンドウサイズ。\n",
    "        window_size_right (int): 右側のコンテキストのウィンドウサイズ。\n",
    "        keyword_positions (list): キーワードの位置情報とファイル名を格納するリスト。\n",
    "        file_name (str): 検索対象のファイル名。\n",
    "\n",
    "    Returns:\n",
    "        list: KWIC結果のリスト。[(left_context, keyword, right_context), ...]\n",
    "    \"\"\"\n",
    "    results = []  # 結果を格納するリスト\n",
    "    for token in doc:\n",
    "        if part_of_speech is None or token.pos_ == part_of_speech:\n",
    "            if token.lemma_.lower() == keyword.lower(): # キーワードが一致する場合(小文字で比較, lemma_属性を使用)\n",
    "                left_context = [t.text_with_ws for t in doc[max(0, token.i - window_size_left):token.i]]\n",
    "                right_context = [t.text_with_ws for t in doc[token.i + 1:token.i + 1 + window_size_right]]\n",
    "\n",
    "                # 結果をリストに追加\n",
    "                results.append((left_context, [token.text], right_context, file_name))\n",
    "                keyword_positions.append((token.i, file_name))\n",
    "    return results\n",
    "\n",
    "def search_phrase(doc, phrase, window_size_left, window_size_right, nlp, keyword_positions, file_name):\n",
    "    \"\"\"\n",
    "    フレーズレベルでキーワードを検索し、KWIC結果を返す関数。\n",
    "\n",
    "    Args:\n",
    "        doc (Doc): spaCyのDocオブジェクト。\n",
    "        phrase (str): 検索するフレーズ。\n",
    "        window_size_left (int): 左側のコンテキストのウィンドウサイズ。\n",
    "        window_size_right (int): 右側のコンテキストのウィンドウサイズ。\n",
    "        nlp (Language): spaCyの言語モデルオブジェクト。\n",
    "        keyword_positions (list): キーワードの位置情報とファイル名を格納するリスト。\n",
    "        file_name (str): 検索対象のファイル名。\n",
    "\n",
    "    Returns:\n",
    "        list: KWIC結果のリスト。[(left_context, keyword, right_context), ...]\n",
    "    \"\"\"\n",
    "    matcher = PhraseMatcher(nlp.vocab, attr='LOWER')  # 小文字でのマッチング\n",
    "    phrase_doc = nlp.make_doc(phrase.lower())  # nlp.make_docを使用してトークナイズのみ実行\n",
    "    matcher.add(\"PHRASE\", [phrase_doc])  # マッチャーにフレーズを追加\n",
    "    matches = matcher(doc)  # マッチを実行\n",
    "\n",
    "    results = []\n",
    "    for match_id, start, end in matches:\n",
    "        # コンテキストをトークンのリストとして取得し、text_with_ws属性を使用して空白情報を含める\n",
    "        left_context_tokens = [token.text_with_ws for token in doc[max(0, start - window_size_left):start]]\n",
    "        right_context_tokens = [token.text_with_ws for token in doc[end:end + window_size_right]]\n",
    "        matched_phrase = [token.text_with_ws for token in doc[start:end]]  # マッチしたフレーズを取得\n",
    "\n",
    "        # 結果をリストに追加\n",
    "        results.append((left_context_tokens, matched_phrase, right_context_tokens, file_name))\n",
    "        keyword_positions.append((start, file_name))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def search_regex(doc, keyword, window_size_left, window_size_right, keyword_positions, file_name):\n",
    "    \"\"\"\n",
    "    正規表現を用いてキーワードを検索し、KWIC結果を返す関数。\n",
    "\n",
    "    Args:\n",
    "        doc (Doc): spaCyのDocオブジェクト。\n",
    "        keyword (str): 検索する正規表現パターン。\n",
    "        window_size_left (int): 左側のコンテキストのウィンドウサイズ。\n",
    "        window_size_right (int): 右側のコンテキストのウィンドウサイズ。\n",
    "        keyword_positions (list): キーワードの位置情報とファイル名を格納するリスト。\n",
    "        file_name (str): 検索対象のファイル名。\n",
    "\n",
    "    Returns:\n",
    "        list: KWIC結果のリスト。[(left_context, keyword, right_context), ...]\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for match in re.finditer(keyword, doc.text): # 正規表現パターンにマッチするキーワードを検索\n",
    "        start_idx = match.start() # マッチするキーワードの開始インデックス\n",
    "        end_idx = match.end()   # マッチするキーワードの終了インデックス\n",
    "        \n",
    "        # 左右のコンテキストを取得\n",
    "        start_token_idx = [token.i for token in doc if token.idx <= start_idx][-1] # start_idxに最も近いトークンのインデックスを取得(キーワードの最初のトークンのインデックス)\n",
    "        end_token_idx = [token.i for token in doc if token.idx < end_idx][-1] # end_idxに最も近いトークンのインデックスを取得(キーワードの最後のトークンのインデックス)\n",
    "\n",
    "        # 左右のコンテキストをトークンのリストとして取得し、text_with_wsを使用\n",
    "        left_context = [t.text_with_ws for t in doc[max(0, start_token_idx - window_size_left):start_token_idx]]\n",
    "        right_context = [t.text_with_ws for t in doc[end_token_idx + 1:end_token_idx + 1 + window_size_right]]\n",
    "        keyword_token = [t.text_with_ws for t in doc[start_token_idx:end_token_idx + 1]] # キーワードのトークンを取得\n",
    "\n",
    "        \n",
    "        results.append((left_context, keyword_token, right_context, file_name))\n",
    "        keyword_positions.append((start_token_idx, file_name))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def perform_kwic_search(doc, keyword, search_type, file_name, keyword_positions, nlp=None, window_size_left=5, window_size_right=5, part_of_speech=None):\n",
    "    \"\"\"\n",
    "    指定された検索タイプに基づいてKWIC検索を実行し、結果を返す関数。\n",
    "\n",
    "    Args:\n",
    "        doc (Doc): spaCyのDocオブジェクト。\n",
    "        keyword (str): 検索するキーワード。\n",
    "        search_type (str): 検索タイプ。'word', 'lemma', 'phrase', 'regex'のいずれか。\n",
    "        file_name (str): 検索対象のファイル名。\n",
    "        keyword_positions (list): キーワードの位置情報とファイル名を格納するリスト。\n",
    "        nlp (Language, optional): spaCyの言語モデルオブジェクト。デフォルトはNone。\n",
    "        window_size_left (int, optional): 左側のコンテキストのウィンドウサイズ。デフォルトは5。\n",
    "        window_size_right (int, optional): 右側のコンテキストのウィンドウサイズ。デフォルトは5。\n",
    "        part_of_speech (str or None, optional): 検索する品詞。デフォルトはNone。\n",
    "\n",
    "    Returns:\n",
    "        tuple: (KWIC結果のリスト, キーワードの位置情報とファイル名のリスト)\n",
    "    \"\"\"\n",
    "\n",
    "    if search_type == 'word':\n",
    "        results = search_word(doc, keyword, part_of_speech, window_size_left, window_size_right, keyword_positions, file_name)\n",
    "    \n",
    "    elif search_type == 'lemma':\n",
    "        results = search_lemma(doc, keyword, part_of_speech, window_size_left, window_size_right, keyword_positions, file_name)\n",
    "    \n",
    "    elif search_type == 'phrase':\n",
    "        results = search_phrase(doc, keyword, window_size_left, window_size_right, nlp, keyword_positions, file_name)\n",
    "    \n",
    "    elif search_type == 'regex':\n",
    "        results = search_regex(doc, keyword, window_size_left, window_size_right, keyword_positions, file_name)\n",
    "    \n",
    "    return results, keyword_positions\n",
    "\n",
    "\n",
    "all_kwic_results = [] # すべてのKWIC結果を格納するリスト(例: [(left_context, keyword, right_context, file_name), ...])\n",
    "keyword_positions = [] # キーワードの位置情報を格納するリスト(例: [(position, file_name), ...])\n",
    "\n",
    "if part_of_speech is not None and (part_of_speech.lower() == \"none\" or part_of_speech == \"\" or search_type in ['phrase', 'regex']):\n",
    "    part_of_speech = None # 品詞が指定されていない場合、Noneに設定\n",
    "\n",
    "if search_type not in [\"word\", \"lemma\", \"phrase\", \"regex\"]: # 検索タイプが不正な場合、エラーを発生\n",
    "    raise ValueError(\"検索タイプは 'word', 'lemma', 'phrase', 'regex' のいずれかである必要があります。\")\n",
    "\n",
    "for file_name, doc in filtered_docs.items():\n",
    "    results, keyword_positions = perform_kwic_search(\n",
    "        doc, input_keyword, search_type, file_name, keyword_positions, nlp, window_size_left, window_size_right, part_of_speech)\n",
    "    all_kwic_results.extend(results)  # KWIC結果をリストに追加\n",
    "\n",
    "\n",
    "# 右から左に書く言語では、right_contextとleft_contextを入れ替える\n",
    "if language_direction == \"rtl\":\n",
    "    all_kwic_results = [(right_context, keyword, left_context, file_name) for left_context, keyword, right_context, file_name in all_kwic_results]\n",
    "\n",
    "def output_kwic_results_to_html(kwic_results, output_html_file, file_info=True):\n",
    "    \"\"\"\n",
    "    KWIC結果をHTMLファイルに出力する関数。\n",
    "\n",
    "    Args:\n",
    "        kwic_results (list): KWIC結果のリスト。[(left_context, keyword, right_context, file_name), ...]\n",
    "        output_html_file (str): 出力するHTMLファイルの名前。\n",
    "        file_info (bool, optional): ファイル情報を出力するかどうか。デフォルトはTrue。\n",
    "    \"\"\"\n",
    "    with open(output_html_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        # HTMLのヘッダーとスタイルシートを書き込み\n",
    "        outfile.write(\"<!DOCTYPE html>\\n<html>\\n<head>\\n<meta charset='UTF-8'>\\n\")\n",
    "        outfile.write('<meta name=\"viewport\" content=\"width=device-width, initial-scale=0.75\">\\n')\n",
    "        outfile.write(\"<title>KWIC Results</title>\\n<style>\\n\")\n",
    "        outfile.write(\"body { font-family: Arial, sans-serif; margin: 20px; }\\n\")\n",
    "        outfile.write(\"table { width: 100%; border-collapse: collapse; }\\n\")\n",
    "        outfile.write(\"th, td { border: 1px solid #999; padding: 0.5rem; }\\n\")\n",
    "        outfile.write(\"td.line-number { text-align: right; font-size: smaller; color: #777; }\\n\")\n",
    "        outfile.write(\"td.left { text-align: right; }\\n\")\n",
    "        outfile.write(\"td.keyword { text-align: center; font-weight: bold; background-color: #e8e8e8; }\\n\")\n",
    "        outfile.write(\"td.right { text-align: left; }\\n\")\n",
    "        outfile.write(\"</style>\\n</head>\\n<body>\\n\")\n",
    "        outfile.write(\"<h1>KWIC Analysis Results</h1>\\n\")\n",
    "        outfile.write(\"<table>\\n<thead>\\n<tr>\\n<th>#</th>\\n<th>Left Context</th>\\n<th>Keyword</th>\\n<th>Right Context</th>\\n</tr>\\n</thead>\\n<tbody>\\n\")\n",
    "\n",
    "        # 結果をテーブルの行として出力\n",
    "        for i, (left_context, kw, right_context, source_file) in enumerate(kwic_results, 1):\n",
    "            left_str = ''.join(left_context).rstrip()\n",
    "            right_str = ''.join(right_context).rstrip()\n",
    "            keyword_str = ''.join(kw).rstrip()\n",
    "            outfile.write(f\"<tr>\\n<td class='line-number'>{i}</td>\\n<td class='left'>{left_str}</td>\\n<td class='keyword'>{keyword_str}</td>\\n<td class='right'>{right_str}</td>\\n</tr>\\n\")\n",
    "\n",
    "        outfile.write(\"</tbody>\\n</table>\\n\")\n",
    "\n",
    "        # ファイル情報のセクションを追加（オプション）\n",
    "        if file_info:\n",
    "            outfile.write(\"<h2>File Information</h2>\\n\")\n",
    "            outfile.write(\"<ul>\\n\")\n",
    "            for i, (_, _, _, filename) in enumerate(kwic_results, 1):\n",
    "                outfile.write(f\"<li>Line {i}: {filename}</li>\\n\")\n",
    "            outfile.write(\"</ul>\\n\")\n",
    "\n",
    "        outfile.write(\"</body>\\n</html>\")\n",
    "\n",
    "if all_kwic_results:\n",
    "    # コンソールに検索結果の概要を表示\n",
    "    print(f\"検索結果: {len(all_kwic_results)}件 (検索モード: {search_type}, キーワード: {input_keyword}, 品詞: {part_of_speech if part_of_speech else 'なし'})\")\n",
    "    \n",
    "    # 検索結果をHTMLファイルに出力する関数を呼び出し\n",
    "    output_kwic_results_to_html(all_kwic_results, output_html_file)\n",
    "    print(f\"検索結果を '{output_html_file}' に出力しました。\")\n",
    "else: \n",
    "    # 検索結果がなかった場合のメッセージをコンソールに表示\n",
    "    print(f\"'{input_keyword}'に一致する結果は見つかりませんでした。(検索モード: {search_type}, 品詞: {part_of_speech if part_of_speech else 'なし'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ソート機能**\n",
    "## ユーザーが設定するパラメーター\n",
    "ソートレベルを指定する（例:[\"K\", \"1R\", \"1L\", \"2R\"]）   \n",
    "リストの最初の要素が主要なソート基準、次の要素が次のソート基準となる。  \n",
    "\"1R\"はキーワードの右側の最初の単語、\"2R\"は右側の2番目の単語を基準にソート。  \n",
    "\"1L\"はキーワードの左側の最後の単語、\"2L\"は左側の最後から2番目の単語を基準にソート。  \n",
    "\"K\"はキーワード自体を基準にソート。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_levels = [\"1K\", \"1R\", \"2R\", \"3R\"]  # ソートのレベルを指定\n",
    "output_sorted_html_file = \"sorted_kwic_results.html\" # ソートされた結果を出力するHTMLファイル名"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ソートの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_kwic_results(kwic_results, sort_levels):\n",
    "    \"\"\"\n",
    "    KWIC分析結果をソートする。\n",
    "\n",
    "    Args:\n",
    "        kwic_results (list): (left_context, keyword, right_context, file_name)のタプルを含むリスト。\n",
    "        sort_levels (list): ソートのレベルを指定するリスト。\n",
    "\n",
    "    Returns:\n",
    "        list: ソートされたKWIC分析結果。\n",
    "    \"\"\"\n",
    "    def get_sort_key(item):\n",
    "        left, kw, right, _ = item\n",
    "        keys = []\n",
    "        for level in sort_levels:\n",
    "            index, direction = int(level[:-1]) - 1, level[-1]  # '1K', '1R', '1L' の形式を解析\n",
    "            \n",
    "            if direction == 'K':\n",
    "                # キーワードリストからソートキーを取得\n",
    "                keys.append(kw[index] if index < len(kw) else \"\")\n",
    "            elif direction == 'R':\n",
    "                # 右コンテキストリストからソートキーを取得\n",
    "                keys.append(right[index] if index < len(right) else \"\")\n",
    "            elif direction == 'L':\n",
    "                # 左コンテキストリストから逆順でソートキーを取得\n",
    "                keys.append(left[-index-1] if index < len(left) else \"\")\n",
    "            \n",
    "        return tuple(keys)\n",
    "    \n",
    "    return sorted(kwic_results, key=get_sort_key)\n",
    "\n",
    "# ソートされた結果をHTMLファイルに出力\n",
    "sorted_kwic_results = sort_kwic_results(all_kwic_results, sort_levels)\n",
    "# 上記セルで定義した関数を使用してHTMLファイルに出力\n",
    "output_kwic_results_to_html(sorted_kwic_results, output_sorted_html_file)\n",
    "print(f\"ソートされた結果が {output_sorted_html_file} に出力されました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **テキストファイル出力機能**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## コンコーダンス検索結果の出力（window＿size）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出力するテキストファイルの名前を定義\n",
    "output_text_file = \"kwic_results.txt\"\n",
    "\n",
    "# テキストファイルに結果を出力\n",
    "with open(output_text_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for left_context, kw, right_context, _ in all_kwic_results:\n",
    "        # コンテキストとキーワードを結合\n",
    "        left_str = ''.join(left_context).rstrip()\n",
    "        right_str = ''.join(right_context).rstrip()\n",
    "        keyword_str = ''.join(kw).rstrip()\n",
    "        outfile.write(f\"{left_str} [{keyword_str}] {right_str}\\n\") # 出力形式は他の形式にも変更可能\n",
    "\n",
    "print(f\"検索結果を {output_text_file} に出力しました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ソート結果の出力（window＿size）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出力するテキストファイルの名前を定義\n",
    "sorted_output_text_file = \"sorted_kwic_results.txt\"\n",
    "\n",
    "# ソートされたKWIC分析結果をテキストファイルに出力\n",
    "with open(sorted_output_text_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for left_context, kw, right_context, _ in sorted_kwic_results:\n",
    "        # コンテキストとキーワードを結合\n",
    "        left_str = ''.join(left_context).rstrip()\n",
    "        right_str = ''.join(right_context).rstrip()\n",
    "        keyword_str = ''.join(kw).rstrip()\n",
    "        # パディングなしで出力\n",
    "        outfile.write(f\"{left_str} [{keyword_str}] {right_str}\\n\")\n",
    "\n",
    "print(f\"ソートされた検索結果を {sorted_output_text_file} に出力しました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文単位での出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出力するテキストファイルの名前を定義\n",
    "sentences_text_file = \"extracted_sentences.txt\"\n",
    "\n",
    "output_sentences = set()  # 出力済みの文を追跡するためのセット\n",
    "duplicated_sentences = []  # 重複する文を追跡するためのリスト\n",
    "\n",
    "# テキストファイルに出力するためのファイルを開く\n",
    "with open(sentences_text_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for position, file_name in keyword_positions:\n",
    "        # 対応するドキュメントを取得\n",
    "        if file_name in filtered_docs:\n",
    "            doc = filtered_docs[file_name]  # ファイル名に対応するDocオブジェクトを取得\n",
    "            for sent in doc.sents:\n",
    "                if position >= sent.start and position < sent.end: # キーワードが含まれる文を見つけたら\n",
    "                    sent_text = str(sent) # 文を文字列に変換\n",
    "                    if sent_text in output_sentences:  # 文が既に出力されている場合、重複リストに追加\n",
    "                        duplicated_sentences.append(sent_text)\n",
    "                    else:\n",
    "                        f.write(sent_text)  # まだ出力されていない文ならファイルに書き込む\n",
    "                        f.write('\\n')  # 改行を挿入\n",
    "                        output_sentences.add(sent_text)  # 出力済みの文を追跡するためのセットに追加\n",
    "                    break  # 一致した文が見つかったらループを抜ける\n",
    "        else:\n",
    "            print(f\"警告: {file_name} に対応するドキュメントが見つかりませんでした。\")\n",
    "\n",
    "print(f\"抽出された文が {sentences_text_file} に出力されました。\")\n",
    "\n",
    "# 重複する文のリストを表示（重複文の数、内容）\n",
    "print(f\"重複する文の数: {len(duplicated_sentences)}\")\n",
    "for sent in duplicated_sentences:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV形式での出力(通常の検索結果)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出力するCSVファイルの名前を定義\n",
    "output_csv_file = \"kwic_results.csv\"\n",
    "\n",
    "# CSVファイルに結果を出力するための関数\n",
    "def output_kwic_to_csv(kwic_results, output_csv_file):\n",
    "    with open(output_csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        # CSVのヘッダーを書き込む\n",
    "        csvwriter.writerow(['Left Context', 'Keyword', 'Right Context'])\n",
    "        \n",
    "        # 各KWIC結果をCSVファイルに書き込む\n",
    "        for left_context, keyword, right_context, _ in kwic_results:\n",
    "            left_str = ''.join(left_context).rstrip()\n",
    "            right_str = ''.join(right_context).rstrip()\n",
    "            keyword_str = ''.join(keyword).rstrip()\n",
    "            csvwriter.writerow([left_str, keyword_str, right_str])\n",
    "\n",
    "# ソートされていない結果を出力\n",
    "output_kwic_to_csv(all_kwic_results, output_csv_file)\n",
    "print(f\"検索結果が {output_csv_file} に出力されました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV形式での出力(ソートした検索結果)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_output_csv_file = \"sorted_kwic_results.csv\"\n",
    "\n",
    "# CSVファイルに結果を出力するための関数\n",
    "def output_kwic_to_csv(kwic_results, output_csv_file):\n",
    "    with open(output_csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        # CSVのヘッダーを書き込む\n",
    "        csvwriter.writerow(['Left Context', 'Keyword', 'Right Context'])\n",
    "        \n",
    "        # 各KWIC結果をCSVファイルに書き込む\n",
    "        for left_context, keyword, right_context, _ in kwic_results:\n",
    "            left_str = ''.join(left_context).rstrip()\n",
    "            right_str = ''.join(right_context).rstrip()\n",
    "            keyword_str = ''.join(keyword).rstrip()\n",
    "            csvwriter.writerow([left_str, keyword_str, right_str])\n",
    "\n",
    "# ソートされた結果を出力\n",
    "output_kwic_to_csv(sorted_kwic_results, sorted_output_csv_file)\n",
    "print(f\"ソートされた検索結果が {sorted_output_csv_file} に出力されました。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
