{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徴語分析スクリプト\n",
    "\n",
    "このスクリプトはspaCyライブラリを使用して事前に解析したテキストデータに対して特徴語分析を行う。N-gram分析、単語リストの作成、Scattertextによる可視化などの機能が含まれる。\n",
    "\n",
    "## 使用方法\n",
    "\n",
    "### 解析データの読み込み\n",
    "\n",
    "1. 解析済みのテキストデータ（.spacyファイル）とそれに対応するテキストファイル（.txt）を用意する。\n",
    "2. スクリプト内の「ユーザーが設定するパラメーター」セクションで以下の変数を設定する。\n",
    " - `main_directory`: 親ディレクトリへの相対パス  \n",
    " - `subdirectories`: 読み込む子ディレクトリ名のリスト  \n",
    " - `nlp_model`: 解析に利用したモデル名\n",
    " - `selected_file_names`: 特定のファイルのみを読み込む場合はここにtxtファイル名を追加（空リストの場合は全ファイルを読み込む）\n",
    "\n",
    "### N-gram分析の実行\n",
    "\n",
    "1. スクリプト内の「ユーザーが設定するパラメーター」セクションで以下の変数を設定する。\n",
    " - `pos_tags`: 出力結果に含める品詞のリスト\n",
    " - `max_ngram`: 出力結果に含める最大のN-gramの長さ\n",
    " - `top_n`: 上位N件の頻度を出力\n",
    " - `stop_words_list`: ストップワードのリスト\n",
    " - `include_space`: n-gramのトークン間にスペースを含めるかどうか\n",
    " - `original_output_file`: オリジナルのn-gramの出力ファイル名  \n",
    " - `lemma_output_file`: 基本形に変換したn-gramの出力ファイル名\n",
    "2. スクリプトを実行すると、指定したファイル名でCSVファイルが出力される。\n",
    "\n",
    "## 分析内容\n",
    "\n",
    "### 1. N-gram分析\n",
    "\n",
    "指定した品詞や最大N-gramの長さに基づいて、単語やN-gramの出現頻度を分析する。結果はCSVファイルとして出力される。\n",
    "\n",
    "### 2. 単語リストの作成\n",
    "\n",
    "解析済みのDocオブジェクトから単語リストを作成し、各単語の原形、品詞、出現回数を記録する。結果はCSVファイルとして出力される。\n",
    "\n",
    "### 3. Scattertext分析 \n",
    "\n",
    "指定した2つのテキスト群の特徴的な単語を可視化するScattertextプロットを作成する。結果はHTMLファイルとして出力される。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ライブラリのインポート(最初に実行)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "from spacy.tokens import DocBin\n",
    "import scattertext as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **解析データの読み込み**\n",
    "## ユーザーが設定するパラメーター"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本設定\n",
    "main_directory = \"processed_data\"  # 親ディレクトリへの相対パス\n",
    "subdirectories = [\"sub1\"]  # 読み込む子ディレクトリ名のリスト\n",
    "nlp_model = \"en_core_web_sm\"  # 解析に利用したモデル名\n",
    "\n",
    "# 特定のファイルのみを読み込む場合は以下のリストにtxtファイル名を追加(空リストの場合は全ファイルを読み込む)\n",
    "selected_file_names = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解析データの読み込みを実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCyの言語モデルをロード\n",
    "nlp = spacy.load(nlp_model)\n",
    "\n",
    "# 解析結果を格納する辞書の初期化\n",
    "docs_dict = {}\n",
    "# フィルタリングされたDocオブジェクトを格納する辞書の初期化\n",
    "filtered_docs = {}\n",
    "\n",
    "# 指定されたサブディレクトリ内のファイルを読み込む\n",
    "for subdir in subdirectories:\n",
    "    directory = os.path.join(main_directory, subdir)\n",
    "    \n",
    "    # サブディレクトリ内のファイルを読み込む\n",
    "    for filename in os.listdir(directory): \n",
    "        if filename.endswith(\".spacy\"):  # .spacyファイルならば\n",
    "            spacy_path = os.path.join(directory, filename)\n",
    "            txt_filename = filename.replace(\".spacy\", \".txt\")  # .spaCyファイルに対応するテキストファイル名を作成\n",
    "            txt_path = os.path.join(directory, txt_filename)  # .spacyファイルに対応するテキストファイルのパス\n",
    "            \n",
    "            # .spacyファイルからDocオブジェクトを読み込む\n",
    "            doc_bin = DocBin().from_disk(spacy_path)  # DocBinオブジェクトを読み込む\n",
    "            docs = list(doc_bin.get_docs(nlp.vocab))  # Docオブジェクトをリストに格納\n",
    "            \n",
    "            # 対応する.txtファイルからファイル名を読み込む\n",
    "            if os.path.exists(txt_path):\n",
    "                with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "                    file_names = [line.strip() for line in f.readlines()]  # ファイル名をリストに格納\n",
    "                \n",
    "                # Docオブジェクトとファイル名の対応を辞書に格納\n",
    "                for doc, fname in zip(docs, file_names):\n",
    "                    docs_dict[fname] = doc\n",
    "            else:\n",
    "                print(f\"警告: {txt_filename} に対応するテキストファイルが存在しません。\")\n",
    "\n",
    "# ユーザーがファイル名を指定した場合、それに対応するDocオブジェクトをフィルタリング\n",
    "if selected_file_names:\n",
    "    missing_files = []  # 存在しないファイル名を格納するリスト\n",
    "    for fname in selected_file_names:\n",
    "        if fname in docs_dict:  # 指定されたファイル名が読み込んだデータに存在する場合\n",
    "            filtered_docs[fname] = docs_dict[fname]\n",
    "        else:\n",
    "            missing_files.append(fname)\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"警告: 次の指定されたファイルは存在しません: {', '.join(missing_files)}\")\n",
    "    if filtered_docs:\n",
    "        print(\"指定された一部のデータが読み込まれました。\")\n",
    "    else:\n",
    "        print(\"指定されたファイルに対応するデータが見つかりませんでした。\")\n",
    "else:\n",
    "    filtered_docs = docs_dict\n",
    "    print(\"全てのデータが読み込まれました。\")\n",
    "print(f\"読み込まれたデータ数: {len(filtered_docs)}\")\n",
    "print(f\"読み込まれたファイル名: {list(filtered_docs.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **N-gram分析**\n",
    "## ユーザーが設定するパラメーター"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = ['NOUN', 'ADJ', 'ADV','VERB', 'PROPN']  # 出力結果に含める品詞のリスト\n",
    "max_ngram = 5    # 出力結果に含める最大のN-gramの長さ\n",
    "top_n = 1000     # 上位N件の頻度を出力\n",
    "include_space = True  # n-gramのトークン間にスペースを含めるかどうか\n",
    "\n",
    "# 単語の集計用のストップワードのリスト\n",
    "word_stop_words_list = [\"%\", \"-\"]  \n",
    "# N-gram用のストップワードのリスト\n",
    "ngram_stop_words_list = ['。', '、', '.', ',', '!', '！', '?', '？', ':', ';', '-', '&', '%', \"'\", '\"', '#', '(', ')', '[', ']', '{', '}', \"　\"]\n",
    "\n",
    "original_output_file = \"n-gram.csv\"  # オリジナルのn-gramの出力ファイル名\n",
    "lemma_output_file = \"n-gram_lemma.csv\"  # 基本形に変換したn-gramの出力ファイル名"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴語分析の実行（オリジナル）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_freq(doc, pos_tags, word_count, stop_words=None):\n",
    "    \"\"\"\n",
    "    単語の頻度をカウントする関数。\n",
    "    \n",
    "    Args:\n",
    "        doc (Doc): spaCyのDocオブジェクト。\n",
    "        pos_tags (list): 出力結果に含める品詞のリスト。\n",
    "        word_count (Counter): 単語の出現頻度を格納するCounterオブジェクト。\n",
    "        stop_words (set, optional): 除外するストップワードのセット。デフォルトはNone。\n",
    "        \n",
    "    Returns:\n",
    "        Counter: 単語の出現頻度を格納したCounterオブジェクト。\n",
    "    \"\"\"\n",
    "    tokenized_texts = [token.text.lower() for token in doc if token.pos_ in pos_tags and token.text.lower() not in stop_words] # 処理中の品詞のトークンを取得(stop_wordsを除く)\n",
    "    word_count.update(tokenized_texts)  # Counterオブジェクトにトークンを追加\n",
    "    return word_count\n",
    "\n",
    "def get_ngram_freq(doc, ngram, separator, ngram_count, stop_words=None):\n",
    "    \"\"\"\n",
    "    N-gramの頻度をカウントする関数。\n",
    "    \n",
    "    Args:\n",
    "        doc (Doc): spaCyのDocオブジェクト。\n",
    "        ngram (int): N-gramの長さ。\n",
    "        separator (str): N-gramのトークン間のセパレータ。\n",
    "        ngram_count (Counter): N-gramの出現頻度を格納するCounterオブジェクト。\n",
    "        stop_words (set, optional): 除外するストップワードのセット。デフォルトはNone。\n",
    "\n",
    "    Returns:\n",
    "        Counter: N-gramの出現頻度を格納したCounterオブジェクト。\n",
    "    \"\"\"\n",
    "    tokenized_texts = [token.text.lower() for token in doc if token.text.lower() not in stop_words]  # 句読点と空白のトークンを除外(注意)\n",
    "    if ngram == 1:\n",
    "        ngram_count.update(tokenized_texts)  # 1-gramの場合はトークンをそのまま追加\n",
    "    else:\n",
    "        # N-gramのリストを作成( 例: 2-gramの場合、['word1 word2', 'word2 word3', ...])\n",
    "        ngrams = [separator.join(tokenized_texts[i:i+ngram]) for i in range(len(tokenized_texts)-ngram+1)]\n",
    "        ngram_count.update(ngrams) # CounterオブジェクトにN-gramを追加\n",
    "    return ngram_count\n",
    "\n",
    "word_stop_words = set(word_stop_words_list)  # 単語のストップワードをセットに変換\n",
    "ngram_stop_words = set(ngram_stop_words_list) # N-gramのストップワードをセットに変換\n",
    "word_counts = {pos_tag: Counter() for pos_tag in pos_tags}  # 品詞ごとの単語の出現頻度を格納する辞書の初期化(例: {'NOUN': Counter(), 'ADJ': Counter(), ...}\n",
    "ngram_counts = {i: Counter() for i in range(1, max_ngram + 1)}  # N-gramの出力結果を格納する辞書の初期化(例: {1: Counter(), 2: Counter(), ...}\n",
    "separator = ' ' if include_space else ''  # N-gramのトークン間のセパレータを設定\n",
    "results = []  # 出力結果（単語の出現頻度とN-gramの出現頻度）を格納するリストの初期化\n",
    "\n",
    "# フィルタリングされたDocオブジェクトに対して単語の頻度とN-gramの頻度をカウント\n",
    "for doc in filtered_docs.values():\n",
    "    for pos_tag in pos_tags: # 品詞ごとに単語の出現頻度をカウント\n",
    "        get_word_freq(doc, [pos_tag], word_counts[pos_tag], word_stop_words)\n",
    "    for i in range(1, max_ngram + 1):  # N-gramごとに出現頻度をカウント\n",
    "        get_ngram_freq(doc, i, separator, ngram_counts[i], ngram_stop_words)\n",
    "\n",
    "# 品詞ごとの単語の出現頻度をデータフレームに変換\n",
    "for pos_tag, count in word_counts.items():\n",
    "    df = pd.DataFrame(count.most_common(top_n), columns=[f'{pos_tag}_word', f'{pos_tag}_count'])\n",
    "    results.append(df)   # 品詞ごとのdfをリストに追加\n",
    "\n",
    "# N-gramの出現頻度をデータフレームに変換\n",
    "for i, count in ngram_counts.items():               \n",
    "    df = pd.DataFrame(count.most_common(top_n), columns=[f'{i}gram_word', f'{i}gram_count'])\n",
    "    results.append(df)  # N-gramのdfをリストに追加\n",
    "\n",
    "pd.concat(results, axis=1).to_csv(original_output_file, index=False)   # リストに格納されたデータフレームを結合してCSVファイルに保存\n",
    "print(f\"CSVファイルが保存されました。ファイル名: {original_output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴語分析の実行（基本形）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_freq(doc, pos_tags, word_count, stop_words=None):\n",
    "    \"\"\"\n",
    "    単語の頻度をカウントする関数。\n",
    "    \n",
    "    Args:\n",
    "        doc (Doc): spaCyのDocオブジェクト。\n",
    "        pos_tags (list): 出力結果に含める品詞のリスト。\n",
    "        word_count (Counter): 単語の出現頻度を格納するCounterオブジェクト。\n",
    "        stop_words (set, optional): 除外するストップワードのセット。デフォルトはNone。\n",
    "        \n",
    "    Returns:\n",
    "        Counter: 単語の出現頻度を格納したCounterオブジェクト。\n",
    "    \"\"\"\n",
    "    tokenized_texts = [token.lemma_.lower() for token in doc if token.pos_ in pos_tags and token.lemma_.lower() not in stop_words]  # lemmatizationを適用（句読点と空白を除外）\n",
    "    word_count.update(tokenized_texts) \n",
    "    return word_count\n",
    "\n",
    "def get_ngram_freq(doc, ngram, separator, ngram_count, stop_words=None):\n",
    "    \"\"\"\n",
    "    N-gramの頻度をカウントする関数。\n",
    "    \n",
    "    Args:\n",
    "        doc (Doc): spaCyのDocオブジェクト。\n",
    "        ngram (int): N-gramの長さ。\n",
    "        separator (str): N-gramのトークン間のセパレータ。\n",
    "        ngram_count (Counter): N-gramの出現頻度を格納するCounterオブジェクト。\n",
    "        stop_words (set, optional): 除外するストップワードのセット。デフォルトはNone。\n",
    "        \n",
    "    Returns:\n",
    "        Counter: N-gramの出現頻度を格納したCounterオブジェクト。\n",
    "    \"\"\"\n",
    "    tokenized_texts = [token.lemma_.lower() for token in doc if token.lemma_.lower() not in stop_words]  # lemmatizationを適用（句読点と空白を除外）\n",
    "    if ngram == 1:\n",
    "        ngram_count.update(tokenized_texts)\n",
    "    else:\n",
    "        ngrams = [separator.join(tokenized_texts[i:i+ngram]) for i in range(len(tokenized_texts)-ngram+1)]\n",
    "        ngram_count.update(ngrams)\n",
    "    return ngram_count\n",
    "\n",
    "word_stop_words = set(word_stop_words_list)\n",
    "ngram_stop_words = set(ngram_stop_words_list)\n",
    "word_counts = {pos_tag: Counter() for pos_tag in pos_tags}      \n",
    "ngram_counts = {i: Counter() for i in range(1, max_ngram + 1)}  \n",
    "separator = ' ' if include_space else ''\n",
    "results = []\n",
    "\n",
    "for doc in filtered_docs.values():\n",
    "    for pos_tag in pos_tags:\n",
    "        get_word_freq(doc, [pos_tag], word_counts[pos_tag], word_stop_words)\n",
    "    for i in range(1, max_ngram + 1):\n",
    "        get_ngram_freq(doc, i, separator, ngram_counts[i], ngram_stop_words)\n",
    "\n",
    "for pos_tag, count in word_counts.items():              \n",
    "    df = pd.DataFrame(count.most_common(top_n), columns=[f'{pos_tag}_word', f'{pos_tag}_count'])\n",
    "    results.append(df)                                   \n",
    "\n",
    "for i, count in ngram_counts.items():               \n",
    "    df = pd.DataFrame(count.most_common(top_n), columns=[f'{i}gram_word', f'{i}gram_count'])\n",
    "    results.append(df)                               \n",
    "\n",
    "pd.concat(results, axis=1).to_csv(lemma_output_file, index=False) \n",
    "print(f\"CSVファイルが保存されました。ファイル名: {lemma_output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **単語リストの作成**\n",
    "## ユーザーが設定するパラメーター"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"word_list.csv\"  # 出力ファイル名"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単語リスト作成を実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_list_and_attributes(doc, word_list):\n",
    "    \"\"\"\n",
    "    単語リストを作成し、各単語の属性を追加する関数。\n",
    "    \n",
    "    Args:\n",
    "        doc (Doc): spaCyのDocオブジェクト。\n",
    "        word_list (defaultdict): 単語リストを格納する辞書。\n",
    "        \n",
    "    Returns:\n",
    "        defaultdict: 単語リストと各単語の属性を格納した辞書。\n",
    "    \"\"\"\n",
    "    for token in doc:\n",
    "        word_key = (token.text.lower(), token.pos_)          # 単語と品詞の組み合わせをキーとして用いる\n",
    "        word_list[word_key]['token'] = token.text.lower()    # tokenを追加\n",
    "        word_list[word_key]['lemma'] = token.lemma_.lower()  # lemmaを追加\n",
    "        word_list[word_key]['pos'] = token.pos_              # posを追加\n",
    "        word_list[word_key]['count'] += 1                    # countを1増やす\n",
    "    return word_list\n",
    "\n",
    "# 単語リストを格納する辞書の初期化(例: {('word', 'NOUN'): {'token': 'word', 'lemma': 'word', 'pos': 'NOUN', 'count': 1}, ...}\n",
    "word_list = defaultdict(lambda: {'token': '', 'lemma': '', 'pos': '', 'count': 0})\n",
    "\n",
    "for doc in filtered_docs.values():\n",
    "    word_list = get_word_list_and_attributes(doc, word_list)\n",
    "\n",
    "# 辞書の値だけを用いてDataFrameを作成\n",
    "df_word_list = pd.DataFrame(list(word_list.values()))\n",
    "\n",
    "# ソートと列の並び替え\n",
    "df_word_list = df_word_list.sort_values(['token', 'pos'])\n",
    "df_word_list = df_word_list[['token', 'lemma', 'pos', 'count']]\n",
    "\n",
    "df_word_list.to_csv(output_file, index=False)\n",
    "print(f\"単語リストのCSVファイルが保存されました。ファイル名: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Scattertext**\n",
    "## ユーザーが設定するパラメーター"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グループ1とグループ2に含めるtxtファイル名のリスト\n",
    "group1 = ['A.txt']  # y軸に表示するグループ1に含めるファイル名のリスト\n",
    "group2 = ['B.txt']  # x軸に表示するグループ2に含めるファイル名のリスト\n",
    "\n",
    "max_terms = 1000  # 表示する単語数\n",
    "minimum_term_frequency = 2  # 最小出現頻度\n",
    "pos_tags = ['PROPN', 'NOUN', 'ADJ', 'ADV']  # 出力結果に含める品詞のリスト\n",
    "output_file_html = \"scattertext_output.html\" # 出力ファイル名\n",
    "\n",
    "custom_stopwords = {}  # ストップワードを指定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scattertext作成の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []  # Scattertextに渡すデータを格納するリストの初期化\n",
    "\n",
    "# 品詞フィルタリングとストップワード除外を適用してテキストを前処理\n",
    "for fname, doc in filtered_docs.items():\n",
    "    if fname not in group1 and fname not in group2:  # グループに含めるファイル名でない場合はスキップ\n",
    "        continue\n",
    "    # テキストを前処理してリストに追加(品詞フィルタリングとストップワード除外を適用)\n",
    "    filtered_text = ' '.join([token.lemma_.lower() for token in doc if token.pos_ in pos_tags and token.lemma_.lower() not in custom_stopwords])\n",
    "    # ファイル名に応じてデータをグループに分けてリストに追加\n",
    "    if fname in group1:\n",
    "        data.append({'text': filtered_text, 'group': 'Group 1'})\n",
    "    elif fname in group2:\n",
    "        data.append({'text': filtered_text, 'group': 'Group 2'})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "if df.empty:\n",
    "    print(\"エラー: 選択されたファイルに対応するデータが見つかりませんでした。\")\n",
    "else:\n",
    "    try:\n",
    "        # Corpusオブジェクトを作成\n",
    "        corpus = st.CorpusFromPandas(\n",
    "            df,\n",
    "            category_col='group',\n",
    "            text_col='text',\n",
    "            nlp=nlp\n",
    "        ).build()\n",
    "\n",
    "        # Scattertextを生成\n",
    "        html = st.produce_scattertext_explorer(\n",
    "            corpus,\n",
    "            category='Group 1',  # y軸カテゴリ\n",
    "            category_name='Group 1',  # y軸ラベル\n",
    "            not_category_name='Group 2',  # x軸ラベル\n",
    "            width_in_pixels=1000,  # 幅\n",
    "            minimum_term_frequency=minimum_term_frequency, # 指定より出現回数の多い単語のみをプロット\n",
    "            max_terms=max_terms  # 表示する単語数\n",
    "        )\n",
    "\n",
    "        with open(output_file_html, \"w\", encoding='utf-8') as file:\n",
    "            file.write(html)\n",
    "        print(f\"Scattertextプロットが '{output_file_html}' に保存されました。\")\n",
    "    except Exception as e:\n",
    "        print(f\"エラー: Scattertextプロットの生成中にエラーが発生しました。詳細: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
