{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 共起語分析スクリプト(Simple)\n",
    "\n",
    "このスクリプトは、spaCyライブラリを使用して事前に解析されたテキストデータに対して共起語分析を行う。文単位での分析、共起範囲を指定した分析、正規表現による柔軟な検索などの機能を含む。\n",
    "\n",
    "## 使用方法\n",
    "\n",
    "### 解析データの読み込み\n",
    "\n",
    "1. 解析済みのテキストデータ（.spacyファイル）とそれに対応するテキストファイル（.txt）を用意する。\n",
    "2. スクリプト内の「ユーザーが設定するパラメーター」セクションで以下の変数を設定する。\n",
    " - `main_directory`: 親ディレクトリへの相対パス  \n",
    " - `subdirectories`: 読み込む子ディレクトリ名のリスト \n",
    " - `nlp_model`: 解析に利用したモデル名\n",
    " - `selected_file_names`: 特定のファイルのみを読み込む場合はここにtxtファイル名を追加（空リストの場合は全ファイルを読み込む）\n",
    "\n",
    "### 共起語分析（文単位）の実行\n",
    "\n",
    "1. スクリプト内の「ユーザーが設定するパラメーター」セクションで以下の変数を設定する。\n",
    " - `search_type`: 検索モード（word, lemma, phrase, regex）\n",
    " - `keyword`: 検索するキーワード\n",
    " - `part_of_speech`: キーワードの品詞（word, lemma検索のみ）\n",
    " - `output_pos`: フィルターする品詞のリスト\n",
    " - `max_output`: 出力する行数\n",
    " - `output_file`: 出力するCSVファイル名\n",
    "2. スクリプトを実行すると、指定したファイル名でCSVファイルが出力される。\n",
    "\n",
    "### 例文検索機能の実行\n",
    "\n",
    "1. スクリプト内の「ユーザーが設定するパラメーター」セクションで以下の変数を設定する。\n",
    " - `lemma_to_extract`: 抽出する単語の原形\n",
    " - `pos_to_extract`: 抽出する単語の品詞\n",
    " - `output_text_file`: 抽出した文を保存するテキストファイル名\n",
    "2. スクリプトを実行すると、指定したファイル名で例文がテキストファイルに出力される。\n",
    "\n",
    "## 分析内容\n",
    "\n",
    "### 1. 共起語分析（文単位） \n",
    "\n",
    "文単位でキーワードを検索し、文内の共起語を分析する。検索モードには単語、原形、フレーズ、正規表現が選択できる。結果はCSVファイルとして出力される。\n",
    "\n",
    "### 2. 共起語分析（共起範囲指定）\n",
    "\n",
    "キーワードの前後の共起範囲を指定して、その範囲内の共起語を分析する。検索モードには単語、原形、フレーズ、正規表現が選択できる。結果はCSVファイルとして出力される。\n",
    "\n",
    "### 3. 例文検索機能\n",
    "\n",
    "共起語分析の結果から、指定した単語と品詞に対応する例文を抽出し、テキストファイルに出力する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ライブラリのインポート(最初に実行)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **解析データの読み込み**\n",
    "## ユーザーが設定するパラメーター"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本設定\n",
    "main_directory = \"processed_data\"  # 親ディレクトリへの相対パス\n",
    "subdirectories = [\"sub1\"]  # 読み込む子ディレクトリ名のリスト\n",
    "nlp_model = \"en_core_web_sm\"  # 解析に利用したモデル名\n",
    "\n",
    "# 特定のファイルのみを読み込む場合は以下のリストにtxtファイル名を追加(空リストの場合は全ファイルを読み込む)\n",
    "selected_file_names = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解析データ読み込みの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCyの言語モデルをロード\n",
    "nlp = spacy.load(nlp_model)\n",
    "\n",
    "# 解析結果を格納する辞書の初期化\n",
    "docs_dict = {}\n",
    "# フィルタリングされたDocオブジェクトを格納する辞書の初期化\n",
    "filtered_docs = {}\n",
    "\n",
    "# 指定されたサブディレクトリ内のファイルを読み込む\n",
    "for subdir in subdirectories:\n",
    "    directory = os.path.join(main_directory, subdir)\n",
    "    \n",
    "    # サブディレクトリ内のファイルを読み込む\n",
    "    for filename in os.listdir(directory): \n",
    "        if filename.endswith(\".spacy\"):  # .spacyファイルならば\n",
    "            spacy_path = os.path.join(directory, filename)\n",
    "            txt_filename = filename.replace(\".spacy\", \".txt\")  # .spaCyファイルに対応するテキストファイル名を作成\n",
    "            txt_path = os.path.join(directory, txt_filename)  # .spacyファイルに対応するテキストファイルのパス\n",
    "            \n",
    "            # .spacyファイルからDocオブジェクトを読み込む\n",
    "            doc_bin = DocBin().from_disk(spacy_path)  # DocBinオブジェクトを読み込む\n",
    "            docs = list(doc_bin.get_docs(nlp.vocab))  # Docオブジェクトをリストに格納\n",
    "            \n",
    "            # 対応する.txtファイルからファイル名を読み込む\n",
    "            if os.path.exists(txt_path):\n",
    "                with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "                    file_names = [line.strip() for line in f.readlines()]  # ファイル名をリストに格納\n",
    "                \n",
    "                # Docオブジェクトとファイル名の対応を辞書に格納\n",
    "                for doc, fname in zip(docs, file_names):\n",
    "                    docs_dict[fname] = doc\n",
    "            else:\n",
    "                print(f\"警告: {txt_filename} に対応するテキストファイルが存在しません。\")\n",
    "\n",
    "# ユーザーがファイル名を指定した場合、それに対応するDocオブジェクトをフィルタリング\n",
    "if selected_file_names:\n",
    "    missing_files = []  # 存在しないファイル名を格納するリスト\n",
    "    for fname in selected_file_names:\n",
    "        if fname in docs_dict:  # 指定されたファイル名が読み込んだデータに存在する場合\n",
    "            filtered_docs[fname] = docs_dict[fname]\n",
    "        else:\n",
    "            missing_files.append(fname)\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"警告: 次の指定されたファイルは存在しません: {', '.join(missing_files)}\")\n",
    "    if filtered_docs:\n",
    "        print(\"指定された一部のデータが読み込まれました。\")\n",
    "    else:\n",
    "        print(\"指定されたファイルに対応するデータが見つかりませんでした。\")\n",
    "else:\n",
    "    filtered_docs = docs_dict\n",
    "    print(\"全てのデータが読み込まれました。\")\n",
    "print(f\"読み込まれたデータ数: {len(filtered_docs)}\")\n",
    "print(f\"読み込まれたファイル名: {list(filtered_docs.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **共起語分析（文単位）**\n",
    "## ユーザーが設定するパラメーター"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本設定\n",
    "search_type = \"phrase\" # word, lemma, phrase, regex のいずれかを指定\n",
    "keyword = \"hello world\"  # 検索するキーワードを設定\n",
    "\n",
    "# 追加設定\n",
    "part_of_speech = \"None\"  # キーワードの品詞を設定(word, lemma検索のみ)\n",
    "\n",
    "# 出力設定\n",
    "output_pos = [\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\", \"ADV\"]  # フィルターする品詞をリストで設定\n",
    "max_output = 150  # 出力する行数を指定\n",
    "output_file = \"sentence_level_collocations.csv\"  # 出力するCSVファイルの名前を設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 共起語分析の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_word(doc, keyword, part_of_speech, collocation_dict, file_name):\n",
    "    \"\"\"\n",
    "    文単位で単語を検索し、共起語を記録する関数。\n",
    "\n",
    "    Args:\n",
    "        doc (Doc): spaCyのDocオブジェクト。\n",
    "        keyword (str): 検索するキーワード。\n",
    "        part_of_speech (str): キーワードの品詞。Noneの場合は品詞を指定しない。\n",
    "        collocation_dict (defaultdict): 共起語を格納する辞書。\n",
    "        file_name (str): 処理中のファイル名。\n",
    "    \"\"\"\n",
    "    for sent in doc.sents:\n",
    "        keyword_found = False\n",
    "        for token in sent:\n",
    "            # 品詞が指定されていない場合、または品詞が一致する場合、かつキーワード(小文字)が一致する場合\n",
    "            if (part_of_speech is None or token.pos_ == part_of_speech) and token.text.lower() == keyword.lower():\n",
    "                keyword_found = True \n",
    "                break # キーワードが見つかったら文の処理を終了\n",
    "                \n",
    "        if keyword_found:\n",
    "            # キーワードにマッチしないトークンのみを抽出してリストに格納\n",
    "            filtered_tokens = [\n",
    "                token for token in sent\n",
    "                if not(\n",
    "                    (part_of_speech is None or token.pos_ == part_of_speech) and\n",
    "                    token.text.lower() == keyword.lower()\n",
    "                )\n",
    "            ]\n",
    "            record_collocations(filtered_tokens, collocation_dict, file_name)\n",
    "\n",
    "def search_lemma(doc, keyword, part_of_speech, collocation_dict, file_name):\n",
    "    \"\"\"\n",
    "    文単位で単語の原形を検索し、共起語を記録する関数。\n",
    "\n",
    "    Args:\n",
    "        doc (Doc): spaCyのDocオブジェクト。\n",
    "        keyword (str): 検索するキーワードの原形。\n",
    "        part_of_speech (str): キーワードの品詞。Noneの場合は品詞を指定しない。\n",
    "        collocation_dict (defaultdict): 共起語を格納する辞書。\n",
    "        file_name (str): 処理中のファイル名。\n",
    "    \"\"\"\n",
    "    for sent in doc.sents:\n",
    "        lemma_found = False\n",
    "        for token in sent:\n",
    "            # 品詞が指定されていない場合、または品詞が一致する場合、かつキーワードの基本形(小文字)が一致する場合\n",
    "            if (part_of_speech is None or token.pos_ == part_of_speech) and token.lemma_.lower() == keyword.lower():\n",
    "                lemma_found = True\n",
    "                break\n",
    "                \n",
    "        if lemma_found:\n",
    "            filtered_tokens = [\n",
    "                token for token in sent\n",
    "                if not(\n",
    "                    (part_of_speech is None or token.pos_ == part_of_speech) and\n",
    "                    token.lemma_.lower() == keyword.lower()\n",
    "                )\n",
    "            ]\n",
    "            record_collocations(filtered_tokens, collocation_dict, file_name)\n",
    "\n",
    "\n",
    "def search_phrase(doc, phrase, collocation_dict, file_name):\n",
    "    \"\"\"\n",
    "    文単位でフレーズを検索し、共起語を記録する関数。\n",
    "\n",
    "    Args:\n",
    "        doc (Doc): spaCyのDocオブジェクト。\n",
    "        phrase (str): 検索するフレーズ。\n",
    "        collocation_dict (defaultdict): 共起語を格納する辞書。\n",
    "        file_name (str): 処理中のファイル名。\n",
    "    \"\"\"\n",
    "    # PhraseMatcherの初期化\n",
    "    matcher = PhraseMatcher(doc.vocab, attr=\"LOWER\")\n",
    "    phrase_patterns = [nlp(phrase)]  # フレーズをDocオブジェクトに変換\n",
    "    matcher.add(\"PHRASE\", phrase_patterns)\n",
    "\n",
    "    # ドキュメント内でフレーズを検索\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        spans = []  # マッチしたフレーズのスパンを記録するリスト\n",
    "\n",
    "        for match_id, start, end in matches:\n",
    "            span = doc[start:end]\n",
    "            if span.sent == sent: # マッチしたフレーズが処理中の文内にある場合, スパンを記録\n",
    "                spans.append((span.start_char - sent.start_char, span.end_char - sent.start_char))\n",
    "\n",
    "        # spansリストに含まれないトークンのみを抽出して共起語を記録\n",
    "        filtered_tokens = [token for token in sent if not any(start <= token.idx - sent.start_char < end for start, end in spans)]\n",
    "        record_collocations(filtered_tokens, collocation_dict, file_name)\n",
    "\n",
    "def search_regex(doc, keyword, collocation_dict, file_name):\n",
    "    \"\"\"\n",
    "    文単位で正規表現によるパターンを検索し、共起語を記録する関数。\n",
    "\n",
    "    Args:\n",
    "        doc (Doc): spaCyのDocオブジェクト。\n",
    "        keyword (str): 検索する正規表現パターン。\n",
    "        collocation_dict (defaultdict): 共起語を格納する辞書。\n",
    "        file_name (str): 処理中のファイル名。\n",
    "    \"\"\"\n",
    "    for sent in doc.sents:\n",
    "        spans = [] # この文で見つかったキーワードのindexを記録するリスト\n",
    "\n",
    "        for match in re.finditer(keyword, sent.text):  # 文内でキーワードにマッチする箇所を探す\n",
    "            start_idx = match.start()\n",
    "            end_idx = match.end()\n",
    "            spans.append((start_idx, end_idx))  # マッチしたキーワードの範囲(開始と終了のインデックス)を記録\n",
    "\n",
    "        # spansリストに含まれないトークンのみを抽出して共起語を記録\n",
    "        filtered_tokens = [token for token in sent if not any(start <= token.idx - sent.start_char < end for start, end in spans)]\n",
    "        record_collocations(filtered_tokens, collocation_dict, file_name)\n",
    "\n",
    "\n",
    "def record_collocations(tokens, collocation_dict, file_name):\n",
    "    \"\"\"\n",
    "    共起語の情報を辞書に記録する関数。\n",
    "\n",
    "    Args:\n",
    "        tokens (list): 共起語のトークンのリスト。\n",
    "        collocation_dict (defaultdict): 共起語を格納する辞書。\n",
    "        file_name (str): 処理中のファイル名。\n",
    "    \"\"\"\n",
    "    # 集計した共起語の情報を辞書に記録\n",
    "    for token in tokens:\n",
    "        key = (token.lemma_, token.pos_)\n",
    "        collocation_dict[key]['lemma'] = token.lemma_\n",
    "        collocation_dict[key]['pos'] = token.pos_\n",
    "        collocation_dict[key]['freq'] += 1\n",
    "        collocation_dict[key]['indexes'].append((file_name, token.i))\n",
    "\n",
    "\n",
    "\n",
    "def perform_collocation_search(doc, keyword, search_type, collocation_dict, part_of_speech, file_name):\n",
    "    \"\"\"\n",
    "    共起語分析を実行する関数。\n",
    "\n",
    "    Args:\n",
    "        doc (Doc): spaCyのDocオブジェクト。\n",
    "        keyword (str): 検索するキーワード。\n",
    "        search_type (str): 検索モード（word, lemma, phrase, regex）。\n",
    "        collocation_dict (defaultdict): 共起語を格納する辞書。\n",
    "        part_of_speech (str): キーワードの品詞。Noneの場合は品詞を指定しない。\n",
    "        file_name (str): 処理中のファイル名。\n",
    "\n",
    "    Raises:\n",
    "        ValueError: 無効な検索モードが指定された場合。\n",
    "    \"\"\"\n",
    "    if search_type == 'word':\n",
    "        search_word(doc, keyword, part_of_speech, collocation_dict, file_name)\n",
    "    \n",
    "    elif search_type == 'lemma':\n",
    "        search_lemma(doc, keyword, part_of_speech, collocation_dict, file_name)\n",
    "    \n",
    "    elif search_type == 'phrase':\n",
    "        search_phrase(doc, keyword, collocation_dict, file_name)\n",
    "    \n",
    "    elif search_type == 'regex':\n",
    "        search_regex(doc, keyword, collocation_dict, file_name)\n",
    "\n",
    "\n",
    "def save_collocations_to_csv(collocation_dict, output_file, output_pos=None, max_output=None):\n",
    "    \"\"\"\n",
    "    共起語の情報をCSVファイルに保存する。\n",
    "\n",
    "    Args:\n",
    "        collocation_dict (defaultdict): 共起語情報を格納する辞書\n",
    "        output_file (str): 出力ファイル名\n",
    "        output_pos (list, optional): 出力する品詞のリスト。デフォルトはNone。\n",
    "        max_output (int, optional): 出力する結果の最大数。デフォルトはNone。\n",
    "    \"\"\"\n",
    "    # 辞書からデータフレームを作成\n",
    "    df = pd.DataFrame.from_dict(collocation_dict, orient='index')\n",
    "    \n",
    "    # 全品詞に対するソート\n",
    "    df_sorted_all = df.sort_values(by='freq', ascending=False)\n",
    "    if max_output is not None:\n",
    "        df_sorted_all = df_sorted_all.head(max_output)\n",
    "    \n",
    "    # 指定された品詞でフィルタリング\n",
    "    dfs_filtered = []\n",
    "    if output_pos:\n",
    "        for pos in output_pos:\n",
    "            df_filtered = df[df['pos'] == pos].sort_values(by='freq', ascending=False)\n",
    "            if max_output is not None:\n",
    "                df_filtered = df_filtered.head(max_output)\n",
    "            df_filtered = df_filtered[['lemma', 'freq']].reset_index(drop=True)\n",
    "            df_filtered.columns = [f'lemma_{pos}', f'freq_{pos}']\n",
    "            dfs_filtered.append(df_filtered)\n",
    "    \n",
    "    # 指定された品詞に基づくデータフレームを結合\n",
    "    df_filtered_combined = pd.concat(dfs_filtered, axis=1)\n",
    "    # 全品詞の共起語データを追加\n",
    "    df_final = pd.concat([df_filtered_combined, df_sorted_all[['lemma', 'pos', 'freq']].reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    # CSVとして保存\n",
    "    df_final.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "# 共起語を格納するための辞書\n",
    "collocation_dict = defaultdict(lambda: {'lemma': '', 'pos': '', 'freq': 0, 'indexes': []})\n",
    "\n",
    "# 検索タイプが無効な場合はエラーを返す\n",
    "if search_type not in [\"word\", \"lemma\", \"regex\", \"phrase\"]:\n",
    "    raise ValueError(\"検索タイプは 'word', 'lemma', 'regex', 'phrase' のいずれかである必要があります。\")\n",
    "    \n",
    "# 品詞が指定されていない場合、検索対象の品詞をNoneに設定, search_typeがphraseまたはregexの場合もNoneに設定\n",
    "if part_of_speech == \"NONE\" or part_of_speech == \"\" or search_type == 'phrase' or search_type == 'regex': \n",
    "    part_of_speech = None\n",
    "\n",
    "# フィルタリングされたDocオブジェクトに対して共起語分析を実行\n",
    "for file_name, doc in filtered_docs.items():\n",
    "    perform_collocation_search(doc, keyword, search_type, collocation_dict, part_of_speech, file_name)\n",
    "\n",
    "# 共起語の総数を計算し、結果を表示\n",
    "total_matches = len(collocation_dict)\n",
    "if total_matches > 0:\n",
    "    # コンソールに共起語分析の結果概要を表示\n",
    "    print(f\"共起語分析の結果: {total_matches}件の共起語が見つかりました。\")\n",
    "    print(f\"検索モード: {search_type}, キーワード: {keyword}, 品詞: {part_of_speech if part_of_speech else 'None'}\")\n",
    "    \n",
    "    # 結果をCSVファイルに出力する関数を呼び出し\n",
    "    save_collocations_to_csv(collocation_dict, output_file, output_pos, max_output)\n",
    "    print(f\"共起語分析の結果が {output_file} に保存されました。\")\n",
    "else:\n",
    "    # 共起語が見つからなかった場合のメッセージをコンソールに表示\n",
    "    print(f\"'{keyword}'に対する共起語は見つかりませんでした。\")\n",
    "    print(f\"検索モード: {search_type}, 品詞: {part_of_speech if part_of_speech else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 例文検索機能\n",
    "## ユーザーが設定するパラメーター"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_to_extract = 'earth' # 抽出する単語を指定 \n",
    "pos_to_extract = 'NOUN'  # 抽出する単語の品詞を指定\n",
    "output_text_file = 'extract_file.txt'  # 抽出した文を保存するファイル名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_by_collocation(filtered_docs, collocation_dict, lemma, pos, output_file):\n",
    "    \"\"\"\n",
    "    指定された共起語と品詞に対応する文を抽出し、テキストファイルに保存する。\n",
    "\n",
    "    Args:\n",
    "        filtered_docs (dict): フィルタリングされたDocオブジェクトの辞書\n",
    "        collocation_dict (defaultdict): 共起語情報を格納する辞書(例 : {(lemma, pos): {'lemma': '', 'pos': '', 'freq': 0, 'indexes': []}, ...}\n",
    "        lemma (str): 抽出する単語のレンマ\n",
    "        pos (str): 抽出する単語の品詞\n",
    "        output_file (str): 抽出した文を保存するファイル名\n",
    "    \"\"\"\n",
    "    # 共起語と品詞に対応するインデックスを取得(例 : indexes = [(file_name, token_index), ...]\n",
    "    indexes = collocation_dict.get((lemma, pos), {}).get('indexes', [])\n",
    "    if not indexes:\n",
    "        print(f\"警告: 指定された共起語 ({lemma}, {pos}) に対応する例文が見つかりませんでした。\")\n",
    "        return\n",
    "    \n",
    "    # 処理された文を追跡するためのセット\n",
    "    processed_sentences = set()\n",
    "\n",
    "    # ドキュメントとテキスト情報のペアを用いて実際の文を抽出\n",
    "    sentences = []\n",
    "    for file_name, doc in filtered_docs.items():\n",
    "        for token in doc:\n",
    "            # トークンのインデックスと文書のテキスト情報を組み合わせたキーを作成\n",
    "            token_key = (file_name, token.i)\n",
    "            if token_key in indexes:\n",
    "                sentence_text = token.sent.text\n",
    "                # 重複を避けるために、すでに処理された文をチェック\n",
    "                if sentence_text not in processed_sentences:\n",
    "                    sentences.append(sentence_text)\n",
    "                    processed_sentences.add(sentence_text)\n",
    "    \n",
    "    # テキストファイルに1行1文の形で出力\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence + '\\n')\n",
    "    print(f\"抽出された文({len(sentences)}件)が {output_file} に保存されました。\")\n",
    "\n",
    "# 文を抽出してテキストファイルに保存\n",
    "extract_sentences_by_collocation(filtered_docs, collocation_dict, lemma_to_extract, pos_to_extract, output_text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **共起語分析（共起範囲指定）**\n",
    "## ユーザーが設定するパラメーター"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本設定\n",
    "search_type = \"phrase\"\n",
    "keyword = \"hello world\"  # 検索するキーワードを設定\n",
    "range_left = 0    # キーワードの左側で検索する範囲を設定\n",
    "range_right = 2   # キーワードの右側で検索する範囲を設定\n",
    "\n",
    "# 追加設定\n",
    "part_of_speech = \"None\"  # キーワードの品詞を設定\n",
    "\n",
    "# 出力設定\n",
    "output_pos = [\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\", \"ADV\"]  # フィルターする品詞をリストで設定\n",
    "max_output = 150 # 出力する行数を指定\n",
    "output_file = \"collocations_within_range.csv\"  # 出力するCSVファイルの名前を設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 共起語分析の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_word(doc, keyword, part_of_speech, collocation_dict, file_name, range_left, range_right):\n",
    "    \"\"\"\n",
    "    キーワードの前後の共起範囲を指定して、単語を検索し、共起語を記録する関数。\n",
    "\n",
    "    Args:\n",
    "        doc (Doc): spaCyのDocオブジェクト。\n",
    "        keyword (str): 検索するキーワード。\n",
    "        part_of_speech (str): キーワードの品詞。Noneの場合は品詞を指定しない。\n",
    "        collocation_dict (defaultdict): 共起語を格納する辞書。\n",
    "        file_name (str): 処理中のファイル名。\n",
    "        range_left (int): キーワードの左側で検索する範囲。\n",
    "        range_right (int): キーワードの右側で検索する範囲。\n",
    "    \"\"\"\n",
    "    for token in doc:\n",
    "        if (part_of_speech is None or token.pos_ == part_of_speech) and token.text.lower() == keyword.lower():\n",
    "            sentence = token.sent\n",
    "            token_index_in_sentence = token.i - sentence.start\n",
    "            relative_start = max(0, token_index_in_sentence - range_left)\n",
    "            relative_end = min(len(sentence), token_index_in_sentence + range_right + 1)\n",
    "\n",
    "            # キーワード自体は除外するが、範囲内の同じ単語は共起語としてカウント\n",
    "            window_tokens = [\n",
    "                sentence[i] for i in range(relative_start, relative_end)\n",
    "                if i != token_index_in_sentence \n",
    "            ]\n",
    "\n",
    "            record_collocations(window_tokens, collocation_dict, file_name)\n",
    "\n",
    "def search_lemma(doc, keyword, part_of_speech, collocation_dict, file_name, range_left, range_right):\n",
    "    \"\"\"\n",
    "    キーワードの前後の共起範囲を指定して、単語の原形を検索し、共起語を記録する関数。\n",
    "\n",
    "    Args:\n",
    "        doc (Doc): spaCyのDocオブジェクト。\n",
    "        keyword (str): 検索するキーワードの原形。\n",
    "        part_of_speech (str): キーワードの品詞。Noneの場合は品詞を指定しない。\n",
    "        collocation_dict (defaultdict): 共起語を格納する辞書。\n",
    "        file_name (str): 処理中のファイル名。\n",
    "        range_left (int): キーワードの左側で検索する範囲。\n",
    "        range_right (int): キーワードの右側で検索する範囲。\n",
    "    \"\"\"\n",
    "    for token in doc:\n",
    "        if (part_of_speech is None or token.pos_ == part_of_speech) and token.lemma_.lower() == keyword.lower():\n",
    "            sentence = token.sent\n",
    "            token_index_in_sentence = token.i - sentence.start\n",
    "            relative_start = max(0, token_index_in_sentence - range_left)\n",
    "            relative_end = min(len(sentence), token_index_in_sentence + range_right + 1)\n",
    "\n",
    "            # キーワードのレンマ自体は除外するが、範囲内の同じレンマは共起語としてカウント\n",
    "            window_tokens = [\n",
    "                sentence[i] for i in range(relative_start, relative_end)\n",
    "                if i != token_index_in_sentence\n",
    "            ]\n",
    "\n",
    "            record_collocations(window_tokens, collocation_dict, file_name)\n",
    "\n",
    "def search_phrase(doc, phrase, collocation_dict, file_name, range_left, range_right):\n",
    "    \"\"\"\n",
    "    キーワードの前後の共起範囲を指定して、フレーズを検索し、共起語を記録する関数。\n",
    "\n",
    "    Args:\n",
    "        doc (Doc): spaCyのDocオブジェクト。\n",
    "        phrase (str): 検索するフレーズ。\n",
    "        collocation_dict (defaultdict): 共起語を格納する辞書。\n",
    "        file_name (str): 処理中のファイル名。\n",
    "        range_left (int): キーワードの左側で検索する範囲。\n",
    "        range_right (int): キーワードの右側で検索する範囲。\n",
    "    \"\"\"\n",
    "    # PhraseMatcherの初期化\n",
    "    matcher = PhraseMatcher(doc.vocab, attr=\"LOWER\")\n",
    "    \n",
    "    # フレーズのパターンをDocオブジェクトとして追加\n",
    "    phrase_patterns = [nlp(phrase)]  # フレーズをDocオブジェクトに変換\n",
    "    matcher.add(\"PHRASE\", phrase_patterns)\n",
    "    \n",
    "    # ドキュメント内でフレーズを検索\n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    # マッチしたフレーズに対する処理\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]  # マッチしたフレーズのスパン\n",
    "        sentence = span.sent  # スパンが含まれる文\n",
    "        \n",
    "        # マッチしたフレーズの開始トークンと終了トークンを基準にウィンドウサイズを適用\n",
    "        window_start = max(span.start - range_left, sentence.start)\n",
    "        window_end = min(span.end + range_right, sentence.end)\n",
    "        \n",
    "        # ウィンドウ内のトークンを抽出（フレーズ自体は除外）\n",
    "        window_tokens = [sentence[i] for i in range(window_start, window_end) if i < span.start or i >= span.end]\n",
    "        \n",
    "        # フィルタリングされたトークンのリストを使用して共起語を記録\n",
    "        record_collocations(window_tokens, collocation_dict, file_name)\n",
    "\n",
    "\n",
    "def search_regex(doc, keyword, collocation_dict, file_name, range_left, range_right):\n",
    "    \"\"\"\n",
    "    キーワードの前後の共起範囲を指定して、正規表現によるパターンを検索し、共起語を記録する関数。\n",
    "\n",
    "    Args:\n",
    "        doc (Doc): spaCyのDocオブジェクト。\n",
    "        keyword (str): 検索する正規表現パターン。\n",
    "        collocation_dict (defaultdict): 共起語を格納する辞書。\n",
    "        file_name (str): 処理中のファイル名。\n",
    "        range_left (int): キーワードの左側で検索する範囲。\n",
    "        range_right (int): キーワードの右側で検索する範囲。\n",
    "    \"\"\"\n",
    "    for sent in doc.sents:\n",
    "        for match in re.finditer(keyword, sent.text):\n",
    "            # キーワードにマッチした部分の文内インデックス範囲を取得\n",
    "            start_idx, end_idx = match.start(), match.end()\n",
    "\n",
    "            # キーワードにマッチしたトークンのインデックスを取得\n",
    "            matched_token_indexes = [\n",
    "                token.i for token in sent\n",
    "                if token.idx >= start_idx + sent.start_char and token.idx < end_idx + sent.start_char\n",
    "            ]\n",
    "\n",
    "            # ウィンドウサイズに基づいてトークンを抽出\n",
    "            window_start = max(matched_token_indexes[0] - range_left, sent.start)\n",
    "            window_end = min(matched_token_indexes[-1] + range_right + 1, sent.end)\n",
    "\n",
    "            # ウィンドウ内のトークンを抽出（キーワード自体は除外）\n",
    "            window_tokens = [\n",
    "                token for token in sent\n",
    "                if token.i >= window_start and token.i < window_end and token.i not in matched_token_indexes\n",
    "            ]\n",
    "\n",
    "            record_collocations(window_tokens, collocation_dict, file_name)\n",
    "\n",
    "\n",
    "def record_collocations(tokens, collocation_dict, file_name):\n",
    "    \"\"\"\n",
    "    共起語の情報を辞書に記録する関数。\n",
    "\n",
    "    Args:\n",
    "        tokens (list): 共起語のトークンのリスト。\n",
    "        collocation_dict (defaultdict): 共起語を格納する辞書。\n",
    "        file_name (str): 処理中のファイル名。\n",
    "    \"\"\"\n",
    "    for token in tokens:\n",
    "        key = (token.lemma_, token.pos_)\n",
    "        collocation_dict[key]['lemma'] = token.lemma_\n",
    "        collocation_dict[key]['pos'] = token.pos_\n",
    "        collocation_dict[key]['freq'] += 1\n",
    "        collocation_dict[key]['indexes'].append((file_name, token.i))\n",
    "\n",
    "def perform_collocation_search(doc, keyword, search_type, collocation_dict, part_of_speech, file_name, range_left, range_right):\n",
    "    \"\"\"\n",
    "    共起語分析を実行する関数。\n",
    "\n",
    "    Args:\n",
    "        doc (Doc): spaCyのDocオブジェクト。\n",
    "        keyword (str): 検索するキーワード。\n",
    "        search_type (str): 検索モード（word, lemma, phrase, regex）。\n",
    "        collocation_dict (defaultdict): 共起語を格納する辞書。\n",
    "        part_of_speech (str): キーワードの品詞。Noneの場合は品詞を指定しない。\n",
    "        file_name (str): 処理中のファイル名。\n",
    "        range_left (int): キーワードの左側で検索する範囲。\n",
    "        range_right (int): キーワードの右側で検索する範囲。\n",
    "\n",
    "    Raises:\n",
    "        ValueError: 無効な検索モードが指定された場合。\n",
    "    \"\"\"\n",
    "    if search_type == 'word':\n",
    "        search_word(doc, keyword, part_of_speech, collocation_dict, file_name, range_left, range_right)\n",
    "    \n",
    "    elif search_type == 'lemma':\n",
    "        search_lemma(doc, keyword, part_of_speech, collocation_dict, file_name, range_left, range_right)\n",
    "    \n",
    "    elif search_type == 'phrase':\n",
    "        search_phrase(doc, keyword, collocation_dict, file_name, range_left, range_right)\n",
    "\n",
    "    elif search_type == 'regex':\n",
    "        search_regex(doc, keyword, collocation_dict, file_name, range_left, range_right)\n",
    "\n",
    "\n",
    "def save_collocations_to_csv(collocation_dict, output_file, output_pos=None, max_output=None):\n",
    "    \"\"\"\n",
    "    共起語の情報をCSVファイルに保存する。\n",
    "\n",
    "    Args:\n",
    "        collocation_dict (defaultdict): 共起語情報を格納する辞書\n",
    "        output_file (str): 出力ファイル名\n",
    "        output_pos (list, optional): 出力する品詞のリスト。デフォルトはNone。\n",
    "        max_output (int, optional): 出力する結果の最大数。デフォルトはNone。\n",
    "    \"\"\"\n",
    "    # 辞書からデータフレームを作成\n",
    "    df = pd.DataFrame.from_dict(collocation_dict, orient='index')\n",
    "    \n",
    "    # 全品詞に対するソート\n",
    "    df_sorted_all = df.sort_values(by='freq', ascending=False)\n",
    "    if max_output is not None:\n",
    "        df_sorted_all = df_sorted_all.head(max_output)\n",
    "    \n",
    "    # 指定された品詞でフィルタリング\n",
    "    dfs_filtered = []\n",
    "    if output_pos:\n",
    "        for pos in output_pos:\n",
    "            df_filtered = df[df['pos'] == pos].sort_values(by='freq', ascending=False)\n",
    "            if max_output is not None:\n",
    "                df_filtered = df_filtered.head(max_output)\n",
    "            df_filtered = df_filtered[['lemma', 'freq']].reset_index(drop=True)\n",
    "            df_filtered.columns = [f'lemma_{pos}', f'freq_{pos}']\n",
    "            dfs_filtered.append(df_filtered)\n",
    "    \n",
    "    # 指定された品詞に基づくデータフレームを結合\n",
    "    df_filtered_combined = pd.concat(dfs_filtered, axis=1)\n",
    "    # 全品詞の共起語データを追加\n",
    "    df_final = pd.concat([df_filtered_combined, df_sorted_all[['lemma', 'pos', 'freq']].reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    # CSVとして保存\n",
    "    df_final.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "# 共起語を格納するための辞書\n",
    "collocation_dict = defaultdict(lambda: {'lemma': '', 'pos': '', 'freq': 0, 'indexes': []})\n",
    "\n",
    "#  品詞が指定されていない場合、検索対象の品詞をNoneに設定, search_typeがphraseまたはregexの場合もNoneに設定\n",
    "if part_of_speech == \"NONE\" or part_of_speech == \"\" or search_type == 'phrase' or search_type == 'regex':\n",
    "    part_of_speech = None\n",
    "\n",
    "# 検索タイプが無効な場合はエラーを返す\n",
    "if search_type not in [\"word\", \"lemma\", \"regex\", \"phrase\"]:\n",
    "    raise ValueError(\"Invalid search type. Please choose from 'word', 'lemma', 'regex', or 'phrase'.\")\n",
    "\n",
    "# フィルタリングされたDocオブジェクトに対して共起語分析を実行\n",
    "for file_name, doc in filtered_docs.items():\n",
    "    perform_collocation_search(doc, keyword, search_type, collocation_dict, part_of_speech, file_name, range_left, range_right)\n",
    "\n",
    "# 共起語の総数を計算し、結果を表示\n",
    "total_matches = len(collocation_dict)\n",
    "if total_matches > 0:\n",
    "    # コンソールに共起語分析の結果概要を表示\n",
    "    print(f\"共起語分析の結果: {total_matches}件の共起語が見つかりました。\")\n",
    "    print(f\"検索モード: {search_type}, キーワード: {keyword}, 品詞: {part_of_speech if part_of_speech else '指定なし'}\")\n",
    "    \n",
    "    # 結果をCSVファイルに出力する関数を呼び出し\n",
    "    save_collocations_to_csv(collocation_dict, output_file, output_pos, max_output)\n",
    "    print(f\"共起語分析の結果が {output_file} に保存されました。\")\n",
    "else:\n",
    "    # 共起語が見つからなかった場合のメッセージをコンソールに表示\n",
    "    print(f\"'{keyword}'に対する共起語は見つかりませんでした。\")\n",
    "    print(f\"検索モード: {search_type}, 品詞: {part_of_speech if part_of_speech else '指定なし'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 例文出力機能\n",
    "## ユーザーが設定するパラメーター"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_to_extract = 'earth' # 抽出する単語を指定 \n",
    "pos_to_extract = 'NOUN'  # 抽出する単語の品詞を指定\n",
    "output_text_file = 'extract_file.txt'  # 抽出した文を保存するファイル名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_by_collocation(filtered_docs, collocation_dict, lemma, pos, output_file):\n",
    "    \"\"\"\n",
    "    指定された共起語と品詞に対応する文を抽出し、テキストファイルに保存する。\n",
    "\n",
    "    Args:\n",
    "        filtered_docs (dict): フィルタリングされたDocオブジェクトの辞書\n",
    "        collocation_dict (defaultdict): 共起語情報を格納する辞書(例 : {(lemma, pos): {'lemma': '', 'pos': '', 'freq': 0, 'indexes': []}, ...}\n",
    "        lemma (str): 抽出する単語のレンマ\n",
    "        pos (str): 抽出する単語の品詞\n",
    "        output_file (str): 抽出した文を保存するファイル名\n",
    "    \"\"\"\n",
    "    # 共起語と品詞に対応するインデックスを取得(例 : indexes = [(file_name, token_index), ...]\n",
    "    indexes = collocation_dict.get((lemma, pos), {}).get('indexes', [])\n",
    "    if not indexes:\n",
    "        print(f\"警告: 指定された共起語 ({lemma}, {pos}) に対応する例文が見つかりませんでした。\")\n",
    "        return\n",
    "    \n",
    "    # 処理された文を追跡するためのセット\n",
    "    processed_sentences = set()\n",
    "\n",
    "    # ドキュメントとテキスト情報のペアを用いて実際の文を抽出\n",
    "    sentences = []\n",
    "    for file_name, doc in filtered_docs.items():\n",
    "        for token in doc:\n",
    "            # トークンのインデックスと文書のテキスト情報を組み合わせたキーを作成\n",
    "            token_key = (file_name, token.i)\n",
    "            if token_key in indexes:\n",
    "                sentence_text = token.sent.text\n",
    "                # 重複を避けるために、すでに処理された文をチェック\n",
    "                if sentence_text not in processed_sentences:\n",
    "                    sentences.append(sentence_text)\n",
    "                    processed_sentences.add(sentence_text)\n",
    "    \n",
    "    # テキストファイルに1行1文の形で出力\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence + '\\n')\n",
    "    print(f\"抽出された文({len(sentences)}件)が {output_file} に保存されました。\")\n",
    "\n",
    "# 文を抽出してテキストファイルに保存\n",
    "extract_sentences_by_collocation(filtered_docs, collocation_dict, lemma_to_extract, pos_to_extract, output_text_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kasou",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
